<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Did you say data? on Did you say data?</title>
    <link>/</link>
    <description>Recent content in Did you say data? on Did you say data?</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 15 Oct 2017 00:00:00 -0400</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Linear regression: Modeling and Assumptions</title>
      <link>/post/linear-regression-modeling-and-assumptions/</link>
      <pubDate>Sun, 09 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/linear-regression-modeling-and-assumptions/</guid>
      <description>&lt;p&gt;Regression analysis is a powerful statistical process to find the relations within a dataset, with the key focus being on relationships between the independent variables (predictors) and a dependent variable (outcome). It can be used to build models for inference or prediction. Among several methods of regression analysis, linear regression sets the basis and is quite widely used for &lt;a href=&#34;https://en.wikipedia.org/wiki/Linear_regression#Applications&#34; target=&#34;_blank&#34;&gt;several real-world applications&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;post/2018-09-05-linear-regression-modeling-and-assumptions_files/2120406.jpg&#34; alt=&#34;Source : Google Images&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Source : Google Images&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;In this post, we will look at building a linear regression model for inference. The dataset we will use is the insurance charges data obtained from &lt;a href=&#34;https://www.kaggle.com/mirichoi0218/insurance/home&#34; target=&#34;_blank&#34;&gt;Kaggle&lt;/a&gt;. This data set consists of 1,338 observations and 7 columns: age, sex, bmi, children, smoker, region and charges.&lt;/p&gt;
&lt;p&gt;The key questions that we would be asking are:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Is there a relationship between medical charges and other variables in the dataset?&lt;/li&gt;
&lt;li&gt;How valid is the model we have built?&lt;/li&gt;
&lt;li&gt;What can we do to improve the model?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We start with importing the required libraries and data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(magrittr)
library(purrr)
library(MASS)
library(car)
library(broom)
library(ggplot2)
library(psych)
library(caret)
library(tidyr)
library(effects)
library(tibble)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;insurance &amp;lt;- read.csv(&amp;#39;~/Documents/CodeWork/medicalCost/insurance.csv&amp;#39;)
summary(insurance)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;      age            sex           bmi           children     smoker    
 Min.   :18.00   female:662   Min.   :15.96   Min.   :0.000   no :1064  
 1st Qu.:27.00   male  :676   1st Qu.:26.30   1st Qu.:0.000   yes: 274  
 Median :39.00                Median :30.40   Median :1.000             
 Mean   :39.21                Mean   :30.66   Mean   :1.095             
 3rd Qu.:51.00                3rd Qu.:34.69   3rd Qu.:2.000             
 Max.   :64.00                Max.   :53.13   Max.   :5.000             
       region       charges     
 northeast:324   Min.   : 1122  
 northwest:325   1st Qu.: 4740  
 southeast:364   Median : 9382  
 southwest:325   Mean   :13270  
                 3rd Qu.:16640  
                 Max.   :63770  &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Some simple observations that can be taken from the summary are:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;The age of participants varies from 18 to 64.&lt;/li&gt;
&lt;li&gt;Around 49.48% of participants are female.&lt;/li&gt;
&lt;li&gt;The bmi of participants ranges from 15.96 to 53.13.&lt;/li&gt;
&lt;li&gt;Only 20.48% of the participants are smokers.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Let’s start with building a linear model. Instead of simple linear regression, where you have one predictor and one outcome, we will go with multiple linear regression, where you have more than one predictors and one outcome.&lt;/p&gt;
&lt;p&gt;Multiple linear regression follows the formula :&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[y = beta_0{}+ beta_1x_1+beta_2x_2+...\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The coefficients in this linear equation denote the magnitude of additive relation between the predictor and the response. In simpler words, keeping everything else fixed, a unit change in x_1 will lead to change of beta_1 in the outcome, and so on.&lt;/p&gt;
&lt;div id=&#34;is-there-a-relationship-between-the-medical-charges-and-the-predictors&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Is there a relationship between the medical charges and the predictors?&lt;/h2&gt;
&lt;p&gt;Our first step is finding if there is any relationship between the outcome and the predictors.&lt;/p&gt;
&lt;p&gt;The null hypothesis would be that there is no relation between any of the predictors and the response, which would be possible when all the regression coefficients for the predictors are equal to zero. The alternate hypothesis would be that atleast one of the predictors has a relation with the outcome, that is the regression coefficient of one of the predictors is non-zero.&lt;/p&gt;
&lt;p&gt;This hypothesis is tested by computing the &lt;a href=&#34;http://www.statisticshowto.com/probability-and-statistics/F%20statistic-value-test/&#34; target=&#34;_blank&#34;&gt;F statistic&lt;/a&gt;. In case of no relationship between the predictor and the response, F statistic will be closer to 1. On the contrary, if the alternate hypothesis is true, the F statistic will be greater than 1. The p-value of F statistic can be used to determine whether the null hypothesis can be rejected or not.&lt;/p&gt;
&lt;p&gt;We will start with fitting a multiple linear regression model using all the predictors:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lm.fit &amp;lt;- lm(formula = charges~., data = insurance)
#Here &amp;#39;.&amp;#39; means we are using all the predictors in the dataset.
summary(lm.fit)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Call:
lm(formula = charges ~ ., data = insurance)

Residuals:
     Min       1Q   Median       3Q      Max 
-11304.9  -2848.1   -982.1   1393.9  29992.8 

Coefficients:
                Estimate Std. Error t value Pr(&amp;gt;|t|)    
(Intercept)     -11938.5      987.8 -12.086  &amp;lt; 2e-16 ***
age                256.9       11.9  21.587  &amp;lt; 2e-16 ***
sexmale           -131.3      332.9  -0.394 0.693348    
bmi                339.2       28.6  11.860  &amp;lt; 2e-16 ***
children           475.5      137.8   3.451 0.000577 ***
smokeryes        23848.5      413.1  57.723  &amp;lt; 2e-16 ***
regionnorthwest   -353.0      476.3  -0.741 0.458769    
regionsoutheast  -1035.0      478.7  -2.162 0.030782 *  
regionsouthwest   -960.0      477.9  -2.009 0.044765 *  
---
Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1

Residual standard error: 6062 on 1329 degrees of freedom
Multiple R-squared:  0.7509,    Adjusted R-squared:  0.7494 
F-statistic: 500.8 on 8 and 1329 DF,  p-value: &amp;lt; 2.2e-16&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A high value of F statistic, with a very low p-value (&amp;lt;2.2e-16), implies that the null hypothesis can be rejected. This means there is a potential relationship between the predictors and the outcome.&lt;/p&gt;
&lt;p&gt;RSE (Residual Standard Error) is the estimate of standard deviation of irreducible error (the error which can’t be reduced even if knew the true regression line; hence, irreducible). In simpler words, it is the average deviation between the actual outcome and the true regression line. Hence, a large value of RSE means a high deviation from the true regression line. As such, RSE is useful in determining the lack of fit of the model to the data. RSE in our model is large (6062), indicating that the model doesn’t fit the data well.&lt;/p&gt;
&lt;p&gt;R-squared measures the proportion of variability in the outcome that can be explained by the predictor, and is always between 0 and 1; the higher the value, the better the model is able to explain the variability in the outcome. However, increase in number of predictors mostly results in an increased value of R-squared due to &lt;a href=&#34;https://en.wikipedia.org/wiki/Coefficient_of_determination#Inflation_of_R2&#34; target=&#34;_blank&#34;&gt;inflation of R-squared&lt;/a&gt;. &lt;a href=&#34;https://en.wikipedia.org/wiki/Coefficient_of_determination#Adjusted_R2&#34; target=&#34;_blank&#34;&gt;Adjusted R-squared&lt;/a&gt; adjusts the value of R-squared to avoid this effect. A high value of adjusted R-squared (0.7494) shows that more than 74% of the variance in the data is being explained by the model. From now on, we will use the term R-squared for adjusted R-squared.&lt;/p&gt;
&lt;p&gt;The Std. Error gives us the average amount that the estimated coefficient of a predictor differs from the actual coefficient of predictor. It can be used to compute the confidence interval of an estimated coefficient, which we will see later.&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;t value&lt;/em&gt; of a predictor tells us how many standard deviations its estimated coefficient is away from 0. &lt;em&gt;Pr (&amp;gt;|t|)&lt;/em&gt; for a predictor is the p-value for the estimated regression coefficient, which is same as saying what is the probability of . A very low p-value (&amp;lt;0.05) for a predictor can be used to infer that there is a relationsip between the predictor and the outcome.&lt;/p&gt;
&lt;p&gt;Our next step should be &lt;a href=&#34;https://en.wikipedia.org/wiki/Regression_validation&#34; target=&#34;_blank&#34;&gt;validation of regression analyis&lt;/a&gt;. This may mean validation of underlying assumptions of the model, checking the structure of model with different predictors, looking for values that have an exceptionally large impact on the regression model, looking for observations that have not been represented well enough in the model, and more.&lt;/p&gt;
&lt;p&gt;In the rest of the post, we will look at some of these methods of model validation and improvement.&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;which-variables-have-a-strong-relation-to-medical-charges&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Which variables have a strong relation to medical charges?&lt;/h2&gt;
&lt;p&gt;Now that we have determined that there is a relation between the predictors and the outcome, our next step would be finding out if all or only some of the predictors are related to the outcome.&lt;/p&gt;
&lt;p&gt;If we look at the p-values of the estimated coefficients above, we see that not all the coefficients are statistically significant (&amp;lt;0.05). This means that only a subset of the predictors are related to the outcome.&lt;/p&gt;
&lt;p&gt;We can look at the individual p-values for selecting the variables. This may not be a problem when the number of predictors (7) is quite small compared to the number of observations (1338). This method won’t, however, work when the number of predictors is greater than the number of observations because of the &lt;a href=&#34;http://www.statisticshowto.com/multiple-testing-problem/&#34; target=&#34;_blank&#34;&gt;multiple testing problem&lt;/a&gt;. In such cases, we would have to use the &lt;a href=&#34;https://en.wikipedia.org/wiki/Feature_selection&#34; target=&#34;_blank&#34;&gt;feature/variable selection&lt;/a&gt; methods, like forward selection, backward selection, or mixed selection.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;static/post/2018-09-05-linear-regression-modeling-and-assumptions_files/RegressionMeme.jpg&#34; alt=&#34;Source: Google Images&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Source: Google Images&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Before jumping on to feature selection using any of these methods, let us try linear regression using the features with significant p-values only.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lm.fit.sel &amp;lt;- lm(charges~age+bmi+children+smoker+region, data = insurance)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We will compare this to &lt;a href=&#34;https://www.stat.ubc.ca/~rollin/teach/643w04/lec/node43.html&#34; target=&#34;_blank&#34;&gt;mixed selection&lt;/a&gt;, which is a combination of forward and backward selection. This can be done in R using the &lt;em&gt;stepAIC()&lt;/em&gt; function, which uses &lt;a href=&#34;https://en.wikipedia.org/wiki/Akaike_information_criterion&#34; target=&#34;_blank&#34;&gt;Akaike Information Criterion&lt;/a&gt; (AIC) to select the best model out of multiple models.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#selecting direction = &amp;quot;both&amp;quot; for mixed selection
step.lm.fit &amp;lt;- stepAIC(lm.fit, direction = &amp;quot;both&amp;quot;, trace = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s compare the two models :&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;step.lm.fit$call&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;lm(formula = charges ~ age + bmi + children + smoker + region, 
    data = insurance)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lm.fit.sel$call&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;lm(formula = charges ~ age + bmi + children + smoker + region, 
    data = insurance)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The model given by stepwise selection is same as the model we got by selecting the predictors with significant p-values (works in this case). You can check the summary of the new model to see if there is any improvement in the model.&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;are-there-any-multicollinear-features&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Are there any multicollinear features?&lt;/h2&gt;
&lt;p&gt;Multicollinearity in multiple regression is a phenomenon in which two or more predictors are highly related to each other, and hence one predictor can be used to predict the value of the other. The problem with multi-collinearity is that it can make it harder to estimate the individual effects of the predictors on the outcome.&lt;/p&gt;
&lt;p&gt;Usually, collinearity between pairs of predictors can be detected using a correlation matrix. However, corrrelation matrix fails, for example, when more than two predictors have high collinearity as a group but don’t have high correlations as pairs.&lt;/p&gt;
&lt;p&gt;A better way to detect multicollinearity is Variance Inflation Factor (VIF). VIF of any predictor is the ratio of variance of its estimated coefficient in the full model to the variance of its estimated coefficient when fit on the outcome only by itself (as in simple linear regression). A VIF of 1 indicates no presence of multicollinearity. Usually, a VIF value of above 5 or 10 is taken as an indicator of multicollinearity. The simplest way of getting rid of multicollinearity in that case is to discard the predictor with high value of VIF.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;vif(step.lm.fit) %&amp;gt;% 
  knitr::kable()&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;GVIF&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Df&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;GVIF^(1/(2*Df))&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;age&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.016188&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.008061&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;bmi&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.104197&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.050808&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;children&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.003714&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.001855&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;smoker&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.006369&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.003179&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;region&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.098869&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.015838&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;None of the predictors in our case has a high value of VIF. Hence, we don’t need to worry about multicollinearity in our case.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;static/post/2018-09-05-linear-regression-modeling-and-assumptions_files/multicollinearity%20meme.jpg&#34; alt=&#34;Source: Google Images&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Source: Google Images&lt;/p&gt;
&lt;/div&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;is-the-relationship-linear&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Is the relationship linear?&lt;/h2&gt;
&lt;p&gt;By applying linear regression, we are assuming that there is a linear relationship between the predictors and the outcome. If the underlying relationship is quite far from linear, then most of the inferences we would make would be doubtful. This also means reduced accuracy of model.&lt;/p&gt;
&lt;p&gt;The non-linearity of the model can be determined using the residual plot. &lt;a href=&#34;http://www.statisticshowto.com/residual/&#34; target=&#34;_blank&#34;&gt;Residual&lt;/a&gt; for any observation is the difference between the actual outcome and the fitted outcome as per the model. For multiple linear regression, we can plot the residuals versus the fitted values. Presence of a pattern in the residual plot would imply a problem with the linear assumption of the model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#type = &amp;quot;rstandard&amp;quot; draws a plot for standardized residuals
residualPlot(step.lm.fit, type = &amp;quot;rstandard&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-09-05-linear-regression-modeling-and-assumptions_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The blue line represents a smooth pattern between the fitted values and the standard residuals. The curve in our case denotes slight non-linearity in our data.&lt;/p&gt;
&lt;p&gt;The non-linearity can be further explored by looking at &lt;a href=&#34;https://www.r-bloggers.com/r-regression-diagnostics-part-1/&#34; target=&#34;_blank&#34;&gt;Component Residual plots&lt;/a&gt; (CR plots). CR plots can be created in R using the function &lt;em&gt;ceresPlots()&lt;/em&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ceresPlots(step.lm.fit)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-09-05-linear-regression-modeling-and-assumptions_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The pink line (residual line) is modelled for the relation between the predictor and the residuals. The blue dashed line (component line) is for the line of best fit. A significant difference between the two lines for a predictor implies that the predictor and the outcome don’t have a linear relationship.&lt;/p&gt;
&lt;p&gt;This kind of inconsistency can be seen in the CR plot for &lt;em&gt;bmi&lt;/em&gt;. Let’s take a closer look:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ceresPlot(step.lm.fit, variable = &amp;#39;bmi&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-09-05-linear-regression-modeling-and-assumptions_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The difference between the component line and the residual line becomes more clear now.&lt;/p&gt;
&lt;p&gt;One of the methods of fixing this is introducing non-linear transformation of predictors of the model. Let’s try adding a non-linear transformation of &lt;em&gt;bmi&lt;/em&gt; to the model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#update() can be used to update an existing model with new requirements
step.lm.fit.new &amp;lt;- update(step.lm.fit, .~.+I(bmi^1.25))

ceresPlots(step.lm.fit.new)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-09-05-linear-regression-modeling-and-assumptions_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The CR plot of bmi no more has a difference between the residual line and the component line. As per the CR plots, the addition of a non-linear transformation of bmi to the existing model fixed the problem.&lt;/p&gt;
&lt;p&gt;We can use ANOVA to check if the new model is significantly better than the previous model. A low p-value (&amp;lt;0.05) for the new model will mean we can conclude that it is better than the previous model:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;anova(step.lm.fit, step.lm.fit.new, test = &amp;quot;F&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Analysis of Variance Table

Model 1: charges ~ age + bmi + children + smoker + region
Model 2: charges ~ age + bmi + children + smoker + region + I(bmi^1.25)
  Res.Df        RSS Df Sum of Sq      F  Pr(&amp;gt;F)  
1   1330 4.8845e+10                              
2   1329 4.8697e+10  1 148484981 4.0524 0.04431 *
---
Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since the model with non-linear transformation of &lt;em&gt;bmi&lt;/em&gt; has a sufficiently low p-value (&amp;lt;0.05), we can conclude that it is better than the previous model.&lt;/p&gt;
&lt;p&gt;Let’s look at the residual plot of this new model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;residualPlot(step.lm.fit.new, type = &amp;quot;rstandard&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-09-05-linear-regression-modeling-and-assumptions_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Looking at the residual plot of the new model, there is not much change in the overall pattern of the standard residuals.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;static/post/2018-09-05-linear-regression-modeling-and-assumptions_files/multiple-regression-more-like-multiple-depression.jpg&#34; alt=&#34;Source: Google Images&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Source: Google Images&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Another method of fixing the problem of non-linearity is introducing an &lt;a href=&#34;https://en.wikipedia.org/wiki/Interaction_(statistics)#In_regression&#34; target=&#34;_blank&#34;&gt;interaction&lt;/a&gt; between some predictors. A person who smokes and has a high bmi may have higher charges as compared to a person who has lower bmi and is a non-smoker. Let’s update the model to introduce an interaction between &lt;em&gt;bmi&lt;/em&gt; and &lt;em&gt;smoker&lt;/em&gt;, and see if that makes a difference:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lm.fit1 &amp;lt;- update(step.lm.fit.new, ~ .+bmi*smoker)

residualPlot(lm.fit1, type = &amp;quot;rstandard&amp;quot;, id=TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-09-05-linear-regression-modeling-and-assumptions_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;anova(step.lm.fit.new, lm.fit1, test = &amp;quot;F&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Analysis of Variance Table

Model 1: charges ~ age + bmi + children + smoker + region + I(bmi^1.25)
Model 2: charges ~ age + bmi + children + smoker + region + I(bmi^1.25) + 
    bmi:smoker
  Res.Df        RSS Df  Sum of Sq      F    Pr(&amp;gt;F)    
1   1329 4.8697e+10                                   
2   1328 3.1069e+10  1 1.7627e+10 753.45 &amp;lt; 2.2e-16 ***
---
Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Not only the relation becomes more linear with less appearance of a pattern in the residual plot, the new model is significantly better than the previous model (without interactions) as can be seen with the p-value (&amp;lt;2.2e-16).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(lm.fit1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Call:
lm(formula = charges ~ age + bmi + children + smoker + region + 
    I(bmi^1.25) + bmi:smoker, data = insurance)

Residuals:
     Min       1Q   Median       3Q      Max 
-11932.6  -1968.0  -1303.7   -311.3  29983.2 

Coefficients:
                  Estimate Std. Error t value Pr(&amp;gt;|t|)    
(Intercept)     -14184.835   4052.086  -3.501 0.000480 ***
age                262.504      9.509  27.607  &amp;lt; 2e-16 ***
bmi               1974.710    659.578   2.994 0.002805 ** 
children           514.273    109.947   4.677  3.2e-06 ***
smokeryes       -20370.230   1644.191 -12.389  &amp;lt; 2e-16 ***
regionnorthwest   -654.158    380.892  -1.717 0.086132 .  
regionsoutheast  -1172.647    382.170  -3.068 0.002196 ** 
regionsouthwest  -1285.747    381.967  -3.366 0.000784 ***
I(bmi^1.25)       -661.819    223.449  -2.962 0.003112 ** 
bmi:smokeryes     1440.617     52.483  27.449  &amp;lt; 2e-16 ***
---
Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1

Residual standard error: 4837 on 1328 degrees of freedom
Multiple R-squared:  0.8415,    Adjusted R-squared:  0.8405 
F-statistic: 783.6 on 9 and 1328 DF,  p-value: &amp;lt; 2.2e-16&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Looking at the summary of the model, the R-squared is higher now (0.8405), with new model explaining more than 84% variance of the data, and the RSE has decreased too (4837).&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;non-constant-variance-of-error-terms&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Non-constant variance of error terms&lt;/h2&gt;
&lt;p&gt;Constant variance (&lt;a href=&#34;https://en.wikipedia.org/wiki/Homoscedasticity&#34; target=&#34;_blank&#34;&gt;homoscedasticity&lt;/a&gt;) of errors is another assumption of a linear regression model. The error terms may, for instance, change with the value of the response variable in case of non-constant variance (heteroscedasticity) of errors. Some of the graphical methods of identifying heteroscedasticity is presence of a funnel shape in the residual plot, or existence of a curve in the residual plot. In the above plot, we don’t see any clear pattern.&lt;/p&gt;
&lt;p&gt;A statistical way is an extension of the Breusch-Pagan Test, available in R as &lt;em&gt;ncvTest()&lt;/em&gt; in the cars package. It assumes a null hypothesis of constant variance of errors against the alternate hypothesis that the error variance changes with the level of the response or with a linear combination of predictors.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Evaluate homoscedasticity
# non-constant error variance test
ncvTest(lm.fit1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Non-constant Variance Score Test 
Variance formula: ~ fitted.values 
Chisquare = 17.90486, Df = 1, p = 2.3223e-05&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A very low p-value (~2.3-05) means the null hypothesis can be rejected. In other words, there is a high chance that the errors have a non-constant variance.&lt;/p&gt;
&lt;p&gt;One of the methods to fix this problem is transformation of the outcome variable.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;yTransformer &amp;lt;- 0.8

trans.lm.fit &amp;lt;- update(lm.fit1, charges^yTransformer~.)

# Evaluate homoscedasticity
# non-constant error variance test
ncvTest(trans.lm.fit)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Non-constant Variance Score Test 
Variance formula: ~ fitted.values 
Chisquare = 0.005724708, Df = 1, p = 0.93969&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;residualPlot(trans.lm.fit, type = &amp;quot;rstandard&amp;quot;, id=T)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-09-05-linear-regression-modeling-and-assumptions_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;A p-value of ~0.94 implies here that we cannot reject the null hypothesis of constant variance of error terms.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;static/post/2018-09-05-linear-regression-modeling-and-assumptions_files/2d3a442e8b01577630a816aea1ba2ac8.jpeg&#34; alt=&#34;Source: Google Images&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Source: Google Images&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;However, there is a slight increase in non-linearity of the model as can be seen in the residual plot.&lt;/p&gt;
&lt;p&gt;This can be fixed further by looking at relations between individual predictors and outcome.&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;correlation-of-error-terms&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Correlation of error terms&lt;/h2&gt;
&lt;p&gt;An important assumption of linear regression model is that the consecutive error terms are uncorrelated. The standard errors of the estimated regression coefficients are calculated on the basis of this assumption. If the consecutive error terms are correlated, the standard errors of the estimated regression coefficients may be much larger.&lt;/p&gt;
&lt;p&gt;We can check the auto-correlation of error terms using the &lt;a href=&#34;https://en.wikipedia.org/wiki/Durbin–Watson_statistic&#34; target=&#34;_blank&#34;&gt;Durbin-Watson test&lt;/a&gt;. The null hypothesis is that the consecutive errors have no auto-correlation. The alternate hypothesis is that the the consecutive errors have a statistically significant correlation:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1)
# Test for Autocorrelated Errors
durbinWatsonTest(trans.lm.fit, max.lag = 5, reps=1000)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt; lag Autocorrelation D-W Statistic p-value
   1    -0.036139510      2.070557   0.216
   2    -0.026396886      2.050927   0.340
   3    -0.009537725      2.017017   0.712
   4    -0.004187672      1.996569   0.972
   5     0.008894177      1.970058   0.680
 Alternative hypothesis: rho[lag] != 0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here we are checking for auto-correlation of residuals for 5 different lags. The p-value for none of the lags is less than 0.05. Hence, we cannot reject the null hypothesis that the consecutive errors are not correlated, concluding that the consecutive errors are not correlated.&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;outliers&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Outliers&lt;/h2&gt;
&lt;p&gt;Outliers are the observations which in some way are quite different from the distribution of the data. With respect to a model, an outlier is an observation whose predicted outcome is much different from the actual value of the outcome.&lt;/p&gt;
&lt;p&gt;Residual Plots (yes, again) can be used to identify outliers. We can look at the the last residual plot above. To make a standard comparison of residuals, we can use standardized residuals as in that plot. Usually, the observations with absolute standard residuals above 3 are possible outliers.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#finding ids of observations with absolute standard residuals of 3+, and order by value in desc order
pot.outliers &amp;lt;- stdres(trans.lm.fit) %&amp;gt;%
  tidy() %&amp;gt;%
  dplyr::filter(abs(x)&amp;gt;3) %&amp;gt;%
  dplyr::arrange(-x)
pot.outliers&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 51 x 2
   names     x
   &amp;lt;chr&amp;gt; &amp;lt;dbl&amp;gt;
 1 517    5.31
 2 1301   5.11
 3 220    4.85
 4 1020   4.67
 5 431    4.62
 6 243    4.59
 7 1207   4.48
 8 527    4.42
 9 1028   4.33
10 937    4.25
# ... with 41 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;outlier.ids &amp;lt;- as.numeric(pot.outliers$names)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;51 observations have an absolute standardized residual greater than 3. That marks approximately 3.8% of the observations. This is a high percentage and further analysis should be done to identify the reason behind outliers (which is a totally different topic). For now, we will remove these outliers and build our model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;clean.insurance &amp;lt;- insurance %&amp;gt;%
  dplyr::slice(-(outlier.ids))

#fitting the model on data after removing the outliers
lm.fit2 &amp;lt;- update(trans.lm.fit, .~., data = clean.insurance) 
summary(lm.fit2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Call:
lm(formula = charges^yTransformer ~ age + bmi + children + smoker + 
    region + I(bmi^1.25) + bmi:smoker, data = clean.insurance)

Residuals:
     Min       1Q   Median       3Q      Max 
-1081.34  -137.51   -73.18    26.35  1739.12 

Coefficients:
                  Estimate Std. Error t value Pr(&amp;gt;|t|)    
(Intercept)     -1405.6648   315.5522  -4.455 9.14e-06 ***
age                34.4895     0.7455  46.262  &amp;lt; 2e-16 ***
bmi               201.4243    51.3600   3.922 9.26e-05 ***
children           65.8613     8.5713   7.684 3.06e-14 ***
smokeryes       -1726.8579   127.7735 -13.515  &amp;lt; 2e-16 ***
regionnorthwest  -103.9364    30.0050  -3.464  0.00055 ***
regionsoutheast  -130.2721    29.9565  -4.349 1.48e-05 ***
regionsouthwest  -124.5118    29.8996  -4.164 3.33e-05 ***
I(bmi^1.25)       -67.1363    17.3982  -3.859  0.00012 ***
bmi:smokeryes     145.8096     4.0753  35.779  &amp;lt; 2e-16 ***
---
Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1

Residual standard error: 371.5 on 1277 degrees of freedom
Multiple R-squared:  0.9245,    Adjusted R-squared:  0.9239 
F-statistic:  1737 on 9 and 1277 DF,  p-value: &amp;lt; 2.2e-16&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can’t compare the R-squared now since the model is built by removing the outliers.&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;interpretations&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Interpretations&lt;/h2&gt;
&lt;p&gt;Let’s look at the actual charges vs fitted values for the model. Before doing that, let’s look at the how the fitted values of the very first model that we created stand against the actual outcomes:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predictions &amp;lt;- predict(lm.fit, insurance, interval = &amp;quot;confidence&amp;quot;) %&amp;gt;%
  tidy()

ggplot(predictions, aes(x=insurance$charges, y=fit))+
  geom_point()+
  geom_smooth(aes(color = &amp;#39;model&amp;#39;))+
  geom_line(aes(x=seq(min(insurance$charges),max(insurance$charges), length.out = 1338), 
                y=seq(min(insurance$charges),max(insurance$charges), length.out = 1338), 
                color = &amp;#39;ideal&amp;#39;))+
  labs(x=&amp;quot;actual charges&amp;quot;, y=&amp;quot;fitted values&amp;quot;) + 
  scale_color_manual(&amp;#39;linear relation&amp;#39;, values = c(&amp;#39;red&amp;#39;, &amp;#39;blue&amp;#39;)) +
  theme(legend.position = c(0.8, 0.2)) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-09-05-linear-regression-modeling-and-assumptions_files/figure-html/unnamed-chunk-21-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The initial model is able to approximate the actual charges below $17,000, but as the actual charges go above $20,000, the gap between actual charges and fitted values keeps increasing. As per the initial model, the actual charges near $50,000 are fitted as somewhere near or below $40,000, and this gap keeps increasing upwards.&lt;/p&gt;
&lt;p&gt;In comparison, this is how the fitted values of the last model look against the actual outcomes:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predictions &amp;lt;- predict(lm.fit2, insurance, interval = &amp;quot;confidence&amp;quot;)^(1/yTransformer) %&amp;gt;%
  tidy()

ggplot(predictions, aes(x=insurance$charges, y=fit))+
  geom_point()+
  geom_smooth(aes(color = &amp;#39;model&amp;#39;))+
  geom_line(aes(x=seq(min(insurance$charges),max(insurance$charges), 
                      length.out = 1338), 
                y=seq(min(insurance$charges),max(insurance$charges), 
                      length.out = 1338), 
                color = &amp;#39;ideal&amp;#39;))+
  labs(x=&amp;quot;actual charges&amp;quot;, y=&amp;quot;fitted values&amp;quot;) + 
  scale_color_manual(&amp;#39;relation&amp;#39;, values = c(&amp;#39;red&amp;#39;, &amp;#39;blue&amp;#39;)) +
  theme(legend.position = c(0.8, 0.2)) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-09-05-linear-regression-modeling-and-assumptions_files/figure-html/unnamed-chunk-22-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This is quite an improvement from the initial model. The fitted values are closer to the actual charges, as can be seen by the gaps between the ideal relation and the modelled relation between the actual charges and fitted values.&lt;/p&gt;
&lt;p&gt;There is still a lot of scope of improvement. This model still has a lot of non-constant variance of errors, as can be seen from the varying deviations of fitted values along the line of the ideal relation.&lt;/p&gt;
&lt;p&gt;We can look at the estimated coefficients of the predictors and their confidence intervals for interpretation on how they define the model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;confint(lm.fit2) %&amp;gt;%
  tidy() %&amp;gt;%
  add_column(coefficients = lm.fit2$coefficients, .after = 2) %&amp;gt;%
  knitr::kable()&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;.rownames&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;X2.5..&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;coefficients&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;X97.5..&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;(Intercept)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-2024.72245&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-1405.66485&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-786.60725&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;age&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;33.02690&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;34.48948&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;35.95205&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;bmi&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;100.66518&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;201.42433&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;302.18349&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;children&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;49.04596&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;65.86134&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;82.67673&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;smokeryes&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-1977.52699&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-1726.85792&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-1476.18886&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;regionnorthwest&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-162.80104&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-103.93644&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-45.07185&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;regionsoutheast&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-189.04134&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-130.27207&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-71.50279&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;regionsouthwest&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-183.16949&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-124.51184&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-65.85419&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;I(bmi^1.25)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-101.26845&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-67.13632&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-33.00419&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;bmi:smokeryes&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;137.81459&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;145.80964&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;153.80468&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;In the above table, the X2.5 and X97.5 mark the lower and upper bounds for the 95% confidence interval of the regression coefficients. These are calculated using the standard errors of the estimated coefficients. As an example, for &lt;em&gt;age&lt;/em&gt;, the estimated coefficient is ~34.49 and the 95% confidence interval lies between ~33.03 and ~35.95. This means that as per the model, keeping everything else fixed, an increase in 1 year of age will result in an increase of 34.49 in the value of &lt;span class=&#34;math display&#34;&gt;\[charges^{0.8}\]&lt;/span&gt; (since we transformed the outcome). However, this is an estimate and hence there is a scope for variation. This variation is accounted for by the confidence interval, denoting that about 95% of the times, the change in the value of charges^0.8 will be between 33.03 and 35.95, keeping everything else fixed.&lt;/p&gt;
&lt;p&gt;This effect can be further viewed by plotting the effect using &lt;em&gt;effects&lt;/em&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#get effects for age
effs &amp;lt;- effect(&amp;#39;age&amp;#39;, mod = lm.fit2, 
       xlevels = list(age=min(insurance$age):max(insurance$age)))

#create a data frame for age and the fitted values, with upper and lower confidence bands
model.effs &amp;lt;- effs[c(&amp;#39;x&amp;#39;, &amp;#39;lower&amp;#39;, &amp;#39;fit&amp;#39;, &amp;#39;upper&amp;#39;)] %&amp;gt;%
  as.data.frame()

model.effs$fit &amp;lt;- model.effs$fit^(1/yTransformer)
model.effs$lower &amp;lt;- model.effs$lower^(1/yTransformer)
model.effs$upper &amp;lt;- model.effs$upper^(1/yTransformer)

ggplot(model.effs, aes(x = age, y = fit)) +
  theme_bw()+
  geom_smooth()+
  geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-09-05-linear-regression-modeling-and-assumptions_files/figure-html/unnamed-chunk-24-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;For an average value of other predictors, insurance charges increase with increase in age.&lt;/p&gt;
&lt;p&gt;More interesting effects can be seen for the interaction between &lt;em&gt;bmi&lt;/em&gt; and &lt;em&gt;smoker&lt;/em&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#get effects for bmi*smoker
effs &amp;lt;- effect(&amp;#39;bmi*smoker&amp;#39;, mod = lm.fit2, 
       xlevels = list(bmi=min(insurance$bmi):max(insurance$bmi)))

#create a data frame for bmi*smoker and the fitted values, with upper and lower confidence bands
model.effs &amp;lt;- effs[c(&amp;#39;x&amp;#39;, &amp;#39;lower&amp;#39;, &amp;#39;fit&amp;#39;, &amp;#39;upper&amp;#39;)] %&amp;gt;%
  as.data.frame()

model.effs$fit &amp;lt;- model.effs$fit^(1/yTransformer)
model.effs$lower &amp;lt;- model.effs$lower^(1/yTransformer)
model.effs$upper &amp;lt;- model.effs$upper^(1/yTransformer)

ggplot(model.effs, aes(x = x.bmi, y = fit)) +
  facet_wrap(~x.smoker)+
  theme_bw()+
  geom_smooth(size=0.5)+
  geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-09-05-linear-regression-modeling-and-assumptions_files/figure-html/unnamed-chunk-25-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Non-smokers, irrespective of their bmi, have mostly low insurance charges for an average value of other predictors. Smokers with low bmi have low insurance charges, though still higher than non-smokers with any value of bmi. Moreover, as their bmi increases, the insurance charges of smokers increases rapidly.&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;The model we have built can be used for inference of how the different predictors influence the outcome.&lt;/p&gt;
&lt;p&gt;It is far from perfect. Their is still presence of non-linearity and non-constant variance of errors. Moreover, the outliers and leverage points should be analyzed further to find a better model. It may not (and most probably won’t) give similar results when used to predict the outcome for new, unseen data. In order to use it for prediction, more concrete measures should be taken for ensuring the accuracy of the model, like cross-validation.&lt;/p&gt;
&lt;p&gt;It still helps by providing good estimations of the significant relations between the predictors and the outcome, which can themselves be used to summarize the data in a more useful and presentful way.&lt;/p&gt;
&lt;hr /&gt;
&lt;div id=&#34;sources&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Sources :&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.kaggle.com/mirichoi0218/insurance/home&#34; class=&#34;uri&#34; target=&#34;_blank&#34;&gt;https://www.kaggle.com/mirichoi0218/insurance/home&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;An Introduction to Statistical Learning and Reasoning&lt;/li&gt;
&lt;li&gt;Wikipedia&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.statmethods.net/stats/rdiagnostics.html&#34; class=&#34;uri&#34; target=&#34;_blank&#34;&gt;https://www.statmethods.net/stats/rdiagnostics.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.statmethods.net/stats/regression.html&#34; class=&#34;uri&#34; target=&#34;_blank&#34;&gt;https://www.statmethods.net/stats/regression.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://datascienceplus.com/how-to-detect-heteroscedasticity-and-rectify-it/&#34; class=&#34;uri&#34; target=&#34;_blank&#34;&gt;https://datascienceplus.com/how-to-detect-heteroscedasticity-and-rectify-it/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.statisticshowto.com/probability-and-statistics/f-statistic-value-test/&#34; class=&#34;uri&#34; target=&#34;_blank&#34;&gt;http://www.statisticshowto.com/probability-and-statistics/f-statistic-value-test/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Asynchronous Gossip Protocol and Push-Sum</title>
      <link>/project/gossip-protocol/</link>
      <pubDate>Wed, 02 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/project/gossip-protocol/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Twitter Engine</title>
      <link>/project/twitter-engine/</link>
      <pubDate>Wed, 02 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/project/twitter-engine/</guid>
      <description>

&lt;h1 id=&#34;overview&#34;&gt;Overview&lt;/h1&gt;

&lt;p&gt;The application can be seen at &lt;a href=&#34;https://github.com/krohitm/Twitter-Simulator&#34; target=&#34;_blank&#34;&gt;https://github.com/krohitm/Twitter-Simulator&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Client-Server&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;A client upon connection is able to write tweets as well as search tweets with hashtags and mentions. Tweets contain randomly generated hashtags. Every tweet contains a random mention of another user, chosen by the simulator. A client can also log itself off and upon login receive its tweets. Also, clients are able to retweet.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Simulator&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The Simulator is a separate process and dictates what requests clients can send.&lt;/li&gt;
&lt;li&gt;The simulator initially spawns a given number(n) of clients. These clients register themselves with the server by sending requests.&lt;/li&gt;
&lt;li&gt;Once the clients are registered, the simulator gives each client the number of users that will follow that user. These numbers are decided using Zipf distribution. In our application, the most subscribed user will have n-1 followers, the second most subscribed has (n-1)/2 no. of followers, and so on, as per the Zipf distribution.&lt;/li&gt;
&lt;li&gt;Once the registrations and subscriptions have been done, the clients, independently, start sending tweets. The rate of tweets sent by each user depends upon the no. of subscribers the client has. The more the number of subscribers of a user, the lower the interval in sending tweets.&lt;/li&gt;
&lt;li&gt;Each user, after a certain number of tweets, sends a random request to the server which may be searching for tweets of all users it has subscribed to, searching for certain hashtags, searching for its mentions, or retweeting a tweet. After the random request, the user continues the same cycle.&lt;/li&gt;
&lt;li&gt;Each user, after a certain number of tweets, logs out of the system for x seconds. During this time, the user doesn’t send out any requests. Once reconnected, the user asks server for the tweets of the users it has subscribed to.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;running-the-project&#34;&gt;Running the Project&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;Run epmd -daemon&lt;/li&gt;
&lt;li&gt;Build the application using mix escript.build&lt;/li&gt;
&lt;li&gt;Run the server using the command: ./project server&lt;/li&gt;
&lt;li&gt;On the same machine, run the simulator using the command: ./project simulator &lt;number of users&gt;
where &lt;number of users&gt; is the number of users you want to simulate. Example: ./project simulator 10000&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;performance&#34;&gt;Performance&lt;/h1&gt;

&lt;p&gt;The tests were run on a 64-bit i5 machine with 8GB RAM.
&lt;img src=&#34;https://user-images.githubusercontent.com/10449636/34135026-205a432a-e42c-11e7-901b-92f75b20b2bb.png&#34; alt=&#34;image&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;database-performance&#34;&gt;Database performance&lt;/h1&gt;

&lt;p&gt;Databases are ETS tables. These tables are public to all the server processes (not the client processes, since client runs on a different node).&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Read concurrency - search requests are handled by the 1000 concurrent read actors.
This has been enabled when we create the ETS tables for tweets.&lt;/li&gt;
&lt;li&gt;Write concurrency - this flag has also been set to true to enable 2 write actors to write at the same time.
These flags allow us to take advantage or Elixir’s concurrency and handle more database reads.&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;architecture-and-notes&#34;&gt;Architecture and Notes&lt;/h1&gt;

&lt;p&gt;The Server has been designed to distribute the load of incoming tweet requests.  Requests coming to the server can be of the following form:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Write a tweet and send it to followers&lt;/li&gt;
&lt;li&gt;Search a particular tweet with certain properties&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The server is an Erlang Node which is connected to client and has the PID of the client processes which are running on a different node.&lt;/p&gt;

&lt;p&gt;The server maintains data about the clients in the form of ETS tables.
Actor on Server:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Read Actor:
At present we have 1000 actors that receive a read request from clients. The server distributes read/search requests to one of these actors.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Write Actor:
We have kept only 2 write actors at the moment that do the job of writing data to the respective database once a tweet request has been received.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>The Intelligent ICU Pilot Study: Using Artificial Intelligence Technology for Autonomous Patient Monitoring</title>
      <link>/publication/intelligent-icu/</link>
      <pubDate>Wed, 25 Apr 2018 00:00:00 -0400</pubDate>
      
      <guid>/publication/intelligent-icu/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Autonomous detection of disruptions in the intensive care unit using deep mask RCNN</title>
      <link>/publication/autonomus-detection/</link>
      <pubDate>Sun, 15 Apr 2018 00:00:00 -0400</pubDate>
      
      <guid>/publication/autonomus-detection/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Twitter Simulator</title>
      <link>/project/twitter-simulator/</link>
      <pubDate>Mon, 09 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/project/twitter-simulator/</guid>
      <description>

&lt;p&gt;Link to demo video: &lt;a href=&#34;https://youtu.be/XlY2eoI5o-8&#34; target=&#34;_blank&#34;&gt;https://youtu.be/XlY2eoI5o-8&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Part 1 of this project can be seen at &lt;a href=&#34;https://github.com/krohitm/Twitter-Simulator&#34; target=&#34;_blank&#34;&gt;https://github.com/krohitm/Twitter-Simulator&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The part 1 of our project had all the functionalities working. In part 2 we have used the exact same server that acts as an API to the Phoenix channels (check architecture section for details). This Server that acts as API can handle all the following:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Tweeting to followers&lt;/li&gt;
&lt;li&gt;Searching tweets - tweets, tweets with hashtags, tweets with mentions&lt;/li&gt;
&lt;li&gt;Retweet a tweet that a user receives&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We have broken down the implementation into two parts:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;A few simple clients, that when connected to localhost, make a socket connection to the server&lt;/li&gt;
&lt;li&gt;Simulator: A socket.js file that opens 1000 websocket connections to the server and does the job of simulating tweets, retweets and the various searches.&lt;br /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;simulator&#34;&gt;Simulator:&lt;/h2&gt;

&lt;p&gt;Simulator runs through the sockets in a loop. That does not mean that only one socket is occupied at a time. The server actors for tweets, retweets, search are constantly running in the background and pushing payloads to active sockets. Thus, our simulation is creating sockets which are constantly active, which is also what we are trying to achieve in our simulation.&lt;/p&gt;

&lt;p&gt;The simulator opens 1000 websockets to the server, each as a new user. These users are then registered, and are given subscribers according to zipf distribution, as in part 1. After this, each of these users send tweets, search for tweets, search for hashtags, and search for mentions, choosing one of these behaviors, randomly. This cycle runs infinitely for each user/websocket.&lt;/p&gt;

&lt;h2 id=&#34;client-side&#34;&gt;Client Side:&lt;/h2&gt;

&lt;p&gt;The server is accessed by running localhost:4000 in the browser. This leads to creation of a websocket, which is basically a new user in our case. A new user can give its username(a new one), subscribe to another user, send tweets, query for its mentions, search for hashtags, query for tweets of users it has subscribed to, and retweet the tweets it can see in its feed. These features can be seen in Fig. 1. An example of application of these features can be seen in Fig. 2.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/13625549/34135581-905fb364-e42f-11e7-9b95-05680bb8d56b.png&#34; alt=&#34;alt text&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Fig. 1: Features available to the user. The user can enter a new username, send tweets, subscribe to users, search for tweets of users subscribed to, search for hashtags, search for its mentions, and retweet a tweet in its feed&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/13625549/34135684-298f40fe-e430-11e7-900f-506fc2ad1cbd.png&#34; alt=&#34;alt text&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Fig.2: The new user has given its username as aditya and subscribed to user rohit. It receives tweets by user rohit (hello world!), can retweet, gets results for query for tweets of followed users, gets tweet when mentioned, gets search results for mentions, and gets search results for hashtags(#wow).&lt;/p&gt;

&lt;h2 id=&#34;running-this-project&#34;&gt;Running this project&lt;/h2&gt;

&lt;p&gt;Note: please hard reload web page or turn of browser Javascript caching for changes to be visible&lt;/p&gt;

&lt;h3 id=&#34;running-simulator&#34;&gt;Running Simulator:&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Go to assets &amp;gt; js &amp;gt; app.js&lt;/li&gt;
&lt;li&gt;At the bottom, comment the line import socket from &amp;ldquo;./single_socket&amp;rdquo; , and uncomment the line import socket from &amp;ldquo;./socket&amp;rdquo;. Continue if already so.&lt;/li&gt;
&lt;li&gt;Currently, 1000 websockets have been setup to run. To change the number of websockets, go to assets &amp;gt; js &amp;gt; socket.js. Go to line 13(let maxClients = 1000) and change the number to desired number of websockets. If you don’t want to change the number of websockets, skip this step.&lt;/li&gt;
&lt;li&gt;From terminal, run the command mix phx.server. Once the files have compiled, go to next step.&lt;/li&gt;
&lt;li&gt;Open a browser and run localhost:4000. This will start the given number of websockets.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;running-individual-client&#34;&gt;Running Individual client:&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Go to assets &amp;gt; js &amp;gt;app.js&lt;/li&gt;
&lt;li&gt;At the bottom, comment the line import socket from &amp;ldquo;./socket&amp;rdquo; , and uncomment the line import socket from &amp;ldquo;./single_socket&amp;rdquo;. Continue if already so.&lt;/li&gt;
&lt;li&gt;From terminal, run the command mix phx.server. Once the files have compiled, go to next step.&lt;/li&gt;
&lt;li&gt;Open a browser and run localhost:4000. You can open multiple clients by opening multiple webpages for the given server.&lt;/li&gt;
&lt;li&gt;Once the webpage opens, give a username to the new user.&lt;/li&gt;
&lt;li&gt;If you have opened multiple clients and assigned usernames to them, you can ask a user to subscribe to some existing user by entering the username of that user in the enter user to subscribe box, and press enter.&lt;/li&gt;
&lt;li&gt;You can type a tweet in send tweet box, and press enter to send a tweet. You can mention existing users in these tweets, and any hashtags that you want.&lt;/li&gt;
&lt;li&gt;When you get a tweet in your feed, you can retweet it by clicking the button retweet under that tweet. The retweeted tweet can be seen in the feed of your followers.&lt;/li&gt;
&lt;li&gt;You can query for tweets of users you have subscribed to, by clicking the search tweets you are subscribed to button.&lt;/li&gt;
&lt;li&gt;You can query for your mentions by clicking the search your mentions button.&lt;/li&gt;
&lt;li&gt;You can search for hashtags by entering a hashtag, with the #symbol, in the search hashtag box, and press enter.&lt;/li&gt;
&lt;li&gt;You can clear the screen by clicking the clear screen button.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;performance-with-web-sockets&#34;&gt;Performance with Web Sockets&lt;/h3&gt;

&lt;p&gt;We connected a maximum of 1000 sockets to the server. Additionally, we were also able to run our simulations on these sockets.&lt;/p&gt;

&lt;h3 id=&#34;json-based-api&#34;&gt;JSON Based API&lt;/h3&gt;

&lt;p&gt;This is not really needed since Phoenix does this for us implicitly. There are two kinds of communication:
Client to Server: While sending this data, we make sure it is in the JSON format
Server to Client: For Phoenix to send payload to client, we need to enclose it in a map, which is implicitly converted by Phoenix in a JSON object and sent to the client.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Exploratory Data Analysis</title>
      <link>/post/exploratory-data-analysis/</link>
      <pubDate>Wed, 28 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/exploratory-data-analysis/</guid>
      <description>&lt;p&gt;New Oxford American Dictionary defines exploration as “the action of traveling in or through an unfamiliar area in order to learn about it.” Analysis is defined as “detailed examination of the elements or structure of something, typically as a basis for discussion or interpretation.” This makes Exploratory data analysis an act of travelling through unfamiliar data for a detailed examination of its structure, for further discussion or interpretation.&lt;/p&gt;
&lt;p&gt;Upon reading the book “Think Stats” by Allen Downey, I loved the approach he uses for data exploration he has given in the book. This post and the rest in the series are highly inspired by the book, and uses the approaches and the datasets presented in the book.&lt;/p&gt;
&lt;p&gt;I have used RMarkdown to write the post,including the code, trying to stick mostly to tidyverse. The goal of this post is to reproduce the work in the book, but in R.&lt;/p&gt;
&lt;p&gt;Let’s start with an example. Your friend rated a movie a 4 star on a scale of a 10, 10 being the highest. Another user rated the movie 3 star on a similar scale. You might start thinking of skipping the movie. However, this conclusion fails because of certain reasons:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;You have only two observations to base your conclusion on. We might need more ratings to conclude if the movie is actually that bad.&lt;/li&gt;
&lt;li&gt;Selection Bias: People who gave a critique on the movie might be speaking up because they particularly hated the movie (and really want to vent it out!), while people who found the movie to be good or even okay didn’t care much to speak about it. This is called &lt;strong&gt;Selection Bias&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Someone might have a preexisting belief that a movie won’t be good and upon its release might rate it low because of that belief. Someone who would have loved the trailer may have given a high rating to the movie, even if the movie might have actually not been good. This is called &lt;strong&gt;Confirmation Bias&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In order to have more reliable conclusions, we need to follow a statistical approach, which includes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Data Collection:&lt;/li&gt;
&lt;li&gt;Descriptive Statistics&lt;/li&gt;
&lt;li&gt;Exploratory data analysis&lt;/li&gt;
&lt;li&gt;Estimation&lt;/li&gt;
&lt;li&gt;Hypothesis Testing&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We will be following this approach in the rest of the series. The data we will use will be from &lt;a href=&#34;http://cdc.gov/nchs/nsfg.htm&#34;&gt;National Survey of Family Growth (NSFG)&lt;/a&gt; conducted by CDC. This survey is intended to gather information on family life, marriage and divorce, pregnancy, infertility, use of contraception, and men’s and women’s health. The survey results are used to plan health services and health education programs, and to do statistical studies of families, fertility, and health.&amp;quot;&lt;/p&gt;
&lt;p&gt;We will use data collected by this survey to investigate questions like whether first babies tend to come late. In order to use this data effectively, we have to understand the design of the study. The NSFG has been conducted seven times; each deployment is called a cycle. We will use data from Cycle 6, which was conducted from January 2002 to March 2003.&lt;/p&gt;
&lt;p&gt;The goal of this study is to draw conclusions about the &lt;em&gt;population&lt;/em&gt;, which in this case would be poeple in the United States aged 15-44. Since it is not always possible to collect data from every person in United States, the data is collected from a subset of the population, known as &lt;em&gt;sample&lt;/em&gt;. The people participating in the survey are called &lt;em&gt;respondents&lt;/em&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(extraDistr)
library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The data source is obtained from &lt;a href=&#34;https://www.cdc.gov/nchs/nsfg/nsfg_cycle6.htm&#34; class=&#34;uri&#34;&gt;https://www.cdc.gov/nchs/nsfg/nsfg_cycle6.htm&lt;/a&gt;. The dataset consists of fixed width files for the data, and stata dictionaries consisting of the metadata for the data files. We will use dct.parser function from &lt;a href=&#34;https://github.com/mrdwab/StataDCTutils/blob/master/R/dct.parser.R&#34;&gt;mrdwab/StataDCTutils&lt;/a&gt; to parse the stata dictionaries and assign the metadata to the data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#helper function to parse Stata dictionary
dct.parser &amp;lt;- function(dct, includes = c(&amp;quot;StartPos&amp;quot;, &amp;quot;StorageType&amp;quot;, &amp;quot;ColName&amp;quot;, 
                                         &amp;quot;ColWidth&amp;quot;, &amp;quot;VarLabel&amp;quot;),
                       preview = FALSE) {
  temp &amp;lt;- readLines(dct)
  temp &amp;lt;- temp[grepl(&amp;quot;_column&amp;quot;, temp)]
  
  if (isTRUE(preview)) {
    head(temp)
  } else {
    possibilities &amp;lt;- c(&amp;quot;StartPos&amp;quot;, &amp;quot;StorageType&amp;quot;, 
                       &amp;quot;ColName&amp;quot;, &amp;quot;ColWidth&amp;quot;, &amp;quot;VarLabel&amp;quot;)
    classes &amp;lt;- c(&amp;quot;numeric&amp;quot;, &amp;quot;character&amp;quot;, &amp;quot;
                 character&amp;quot;, &amp;quot;numeric&amp;quot;, &amp;quot;character&amp;quot;)
    pattern &amp;lt;- c(StartPos = &amp;quot;.*\\(([0-9 ]+)\\)&amp;quot;,
                 StorageType = &amp;quot;(byte|int|long|float|double|str[0-9]+)&amp;quot;,
                 ColName = &amp;quot;(.*)&amp;quot;,
                 ColWidth = &amp;quot;%([0-9.]+)[a-z]+&amp;quot;,
                 VarLabel = &amp;quot;(.*)&amp;quot;)
    
    mymatch &amp;lt;- match(includes, possibilities)
    
    pattern &amp;lt;- paste(paste(pattern[mymatch], 
                           collapse =&amp;quot;\\s+&amp;quot;), &amp;quot;$&amp;quot;, sep = &amp;quot;&amp;quot;)    
    
    metadata &amp;lt;- setNames(lapply(seq_along(mymatch), function(x) {
      out &amp;lt;- gsub(pattern, paste(&amp;quot;\\&amp;quot;, x, sep = &amp;quot;&amp;quot;), temp)
      out &amp;lt;- gsub(&amp;quot;^\\s+|\\s+$&amp;quot;, &amp;quot;&amp;quot;, out)
      out &amp;lt;- gsub(&amp;#39;\&amp;quot;&amp;#39;, &amp;quot;&amp;quot;, out, fixed = TRUE)
      class(out) &amp;lt;- classes[mymatch][x] ; out }), 
                         possibilities[mymatch])
    
    implicit.dec &amp;lt;- grepl(&amp;quot;\\.[1-9]&amp;quot;, metadata[[&amp;quot;ColWidth&amp;quot;]])
    if (any(implicit.dec)) {
      message(&amp;quot;Some variables may need to be corrected for implicit decimals. 
              Try &amp;#39;MESSAGES(output_from_dct.parser)&amp;#39; for more details.&amp;quot;)
      metadata[[&amp;quot;Decimals&amp;quot;]] &amp;lt;- rep(NA, length(metadata[[&amp;quot;ColWidth&amp;quot;]]))
      metadata[[&amp;quot;Decimals&amp;quot;]][implicit.dec] &amp;lt;-
        as.numeric(gsub(&amp;quot;[0-9]+\\.&amp;quot;, &amp;quot;&amp;quot;, 
                        metadata[[&amp;quot;ColWidth&amp;quot;]][implicit.dec]))
      metadata[[&amp;quot;ColWidth&amp;quot;]] &amp;lt;- floor(as.numeric(metadata[[&amp;quot;ColWidth&amp;quot;]]))
    }
    
    metadata[[&amp;quot;ColName&amp;quot;]] &amp;lt;- make.names(
      gsub(&amp;quot;\\s&amp;quot;, &amp;quot;&amp;quot;, metadata[[&amp;quot;ColName&amp;quot;]]))
    
    metadata &amp;lt;- data.frame(metadata)
    
    if (&amp;quot;StorageType&amp;quot; %in% includes) {
      metadata &amp;lt;- 
        within(metadata, {
          colClasses &amp;lt;- ifelse(
            StorageType == &amp;quot;byte&amp;quot;, &amp;quot;raw&amp;quot;,
            ifelse(StorageType %in% c(&amp;quot;double&amp;quot;, &amp;quot;long&amp;quot;, &amp;quot;float&amp;quot;), 
                   &amp;quot;numeric&amp;quot;, 
                   ifelse(StorageType == &amp;quot;int&amp;quot;, &amp;quot;integer&amp;quot;,
                          ifelse(substr(StorageType, 1, 3) == &amp;quot;str&amp;quot;, 
                                 &amp;quot;character&amp;quot;, NA))))
        })
    }
    if (any(implicit.dec)) {
      attr(metadata, &amp;quot;MESSAGE&amp;quot;) &amp;lt;- c(sprintf(&amp;quot;%s&amp;quot;, paste(
        &amp;quot;Some variables might need to be corrected for implicit decimals. 
        A variable, &amp;#39;Decimals&amp;#39;, has been created in the metadata that
        indicates the number of decimal places the variable should hold. 
        To correct the output, try (where your stored output is &amp;#39;mydf&amp;#39;): 
        
        lapply(seq_along(mydf[!is.na(Decimals)]), 
        function(x) mydf[!is.na(Decimals)][x]
        / 10^Decimals[!is.na(Decimals)][x])
        
        The variables in question are:
        &amp;quot;)), sprintf(&amp;quot;%s&amp;quot;, metadata[[&amp;quot;ColName&amp;quot;]][!is.na(metadata[[&amp;quot;Decimals&amp;quot;]])]))
            class(attr(metadata, &amp;quot;MESSAGE&amp;quot;)) &amp;lt;- c(
                &amp;quot;MESSAGE&amp;quot;, class(attr(metadata, &amp;quot;MESSAGE&amp;quot;)))
        }
        attr(metadata, &amp;quot;original.dictionary&amp;quot;) &amp;lt;- 
            c(dct, basename(dct))
        metadata
    }
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can read the coulmns from 2002FemPreg.dct and use those columns to import the data from the fixed width file 2002FemPreg.dat&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;femPreg2002columns &amp;lt;- dct.parser(&amp;#39;~/Documents/CodeWork/ThinkStats2/code/2002FemPreg.dct&amp;#39;)
femPreg2002 &amp;lt;- read.fwf(&amp;#39;~/Documents/CodeWork/ThinkStats2/code/2002FemPreg.dat&amp;#39;, widths = femPreg2002columns$ColWidth, col.names = femPreg2002columns$ColName)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Taking a look at the data&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#head(femPreg2002)
femPreg2002 %&amp;gt;%
  summary()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;     caseid         pregordr        howpreg_n       howpreg_p    
 Min.   :    1   Min.   : 1.000   Min.   : 0.00   Min.   :1.000  
 1st Qu.: 3022   1st Qu.: 1.000   1st Qu.: 5.00   1st Qu.:1.000  
 Median : 6161   Median : 2.000   Median : 9.00   Median :1.000  
 Mean   : 6217   Mean   : 2.349   Mean   :15.14   Mean   :1.344  
 3rd Qu.: 9423   3rd Qu.: 3.000   3rd Qu.:23.00   3rd Qu.:2.000  
 Max.   :12571   Max.   :19.000   Max.   :99.00   Max.   :2.000  
                                  NA&amp;#39;s   :13241   NA&amp;#39;s   :13244  
    moscurrp        nowprgdk        pregend1       pregend2    
 Min.   :0.000   Min.   :1.000   Min.   :1.00   Min.   :1.000  
 1st Qu.:2.000   1st Qu.:1.000   1st Qu.:3.00   1st Qu.:3.000  
 Median :5.000   Median :1.000   Median :6.00   Median :4.000  
 Mean   :4.648   Mean   :3.667   Mean   :4.65   Mean   :4.056  
 3rd Qu.:7.000   3rd Qu.:5.000   3rd Qu.:6.00   3rd Qu.:6.000  
 Max.   :9.000   Max.   :9.000   Max.   :9.00   Max.   :6.000  
 NA&amp;#39;s   :13241   NA&amp;#39;s   :13590   NA&amp;#39;s   :352    NA&amp;#39;s   :13575  
    nbrnaliv        multbrth        cmotpreg      prgoutcome   
 Min.   :1.000   Min.   :1.000   Min.   : 864   Min.   :1.000  
 1st Qu.:1.000   1st Qu.:1.000   1st Qu.:1054   1st Qu.:1.000  
 Median :1.000   Median :1.000   Median :1129   Median :1.000  
 Mean   :1.026   Mean   :1.834   Mean   :1379   Mean   :1.351  
 3rd Qu.:1.000   3rd Qu.:1.000   3rd Qu.:1188   3rd Qu.:2.000  
 Max.   :9.000   Max.   :5.000   Max.   :9999   Max.   :3.000  
 NA&amp;#39;s   :4445    NA&amp;#39;s   :13430   NA&amp;#39;s   :9539   NA&amp;#39;s   :38     
    cmprgend       flgdkmo1       cmprgbeg       ageatend    
 Min.   : 833   Min.   :0.00   Min.   : 824   Min.   : 5.00  
 1st Qu.:1071   1st Qu.:0.00   1st Qu.:1065   1st Qu.:18.00  
 Median :1133   Median :0.00   Median :1127   Median :22.00  
 Mean   :1214   Mean   :0.35   Mean   :1116   Mean   :25.93  
 3rd Qu.:1185   3rd Qu.:1.00   3rd Qu.:1181   3rd Qu.:26.00  
 Max.   :9999   Max.   :7.00   Max.   :9997   Max.   :99.00  
 NA&amp;#39;s   :390    NA&amp;#39;s   :9537   NA&amp;#39;s   :180    NA&amp;#39;s   :12462  
    hpageend       gestasun_m       gestasun_w       wksgest     
 Min.   : 6.00   Min.   : 0.000   Min.   : 0.00   Min.   : 0.00  
 1st Qu.:21.00   1st Qu.: 0.000   1st Qu.: 0.00   1st Qu.:13.00  
 Median :26.00   Median : 4.000   Median : 1.00   Median :39.00  
 Mean   :29.95   Mean   : 5.537   Mean   :11.04   Mean   :30.01  
 3rd Qu.:32.00   3rd Qu.: 9.000   3rd Qu.:12.00   3rd Qu.:39.00  
 Max.   :99.00   Max.   :99.000   Max.   :99.00   Max.   :97.00  
 NA&amp;#39;s   :9539    NA&amp;#39;s   :390      NA&amp;#39;s   :390     NA&amp;#39;s   :542    
    mosgest          dk1gest         dk2gest         dk3gest     
 Min.   : 0.000   Min.   :2       Min.   :1.000   Min.   :1.00   
 1st Qu.: 3.000   1st Qu.:2       1st Qu.:5.000   1st Qu.:1.00   
 Median : 9.000   Median :2       Median :5.000   Median :1.00   
 Mean   : 6.858   Mean   :2       Mean   :4.657   Mean   :2.43   
 3rd Qu.: 9.000   3rd Qu.:2       3rd Qu.:5.000   3rd Qu.:2.00   
 Max.   :97.000   Max.   :2       Max.   :5.000   Max.   :9.00   
 NA&amp;#39;s   :408      NA&amp;#39;s   :13592   NA&amp;#39;s   :13558   NA&amp;#39;s   :13472  
 bpa_bdscheck1   bpa_bdscheck2   bpa_bdscheck3      babysex     
 Min.   :0.000   Min.   :0       Min.   :0       Min.   :1.000  
 1st Qu.:0.000   1st Qu.:0       1st Qu.:0       1st Qu.:1.000  
 Median :0.000   Median :0       Median :0       Median :1.000  
 Mean   :0.002   Mean   :0       Mean   :0       Mean   :1.495  
 3rd Qu.:0.000   3rd Qu.:0       3rd Qu.:0       3rd Qu.:2.000  
 Max.   :2.000   Max.   :0       Max.   :0       Max.   :9.000  
 NA&amp;#39;s   :4445    NA&amp;#39;s   :13464   NA&amp;#39;s   :13586   NA&amp;#39;s   :4449   
  birthwgt_lb      birthwgt_oz        lobthwgt        babysex2    
 Min.   : 0.000   Min.   : 0.000   Min.   :1.000   Min.   :1.000  
 1st Qu.: 6.000   1st Qu.: 3.000   1st Qu.:1.000   1st Qu.:1.000  
 Median : 7.000   Median : 7.000   Median :1.000   Median :1.000  
 Mean   : 7.431   Mean   : 7.404   Mean   :2.194   Mean   :1.607  
 3rd Qu.: 8.000   3rd Qu.:11.000   3rd Qu.:2.000   3rd Qu.:2.000  
 Max.   :99.000   Max.   :99.000   Max.   :9.000   Max.   :9.000  
 NA&amp;#39;s   :4449     NA&amp;#39;s   :4506     NA&amp;#39;s   :13526   NA&amp;#39;s   :13430  
  birthwgt_lb2     birthwgt_oz2      lobthwgt2        babysex3    
 Min.   : 0.000   Min.   : 0.000   Min.   :2.0     Min.   :1.00   
 1st Qu.: 4.000   1st Qu.: 2.000   1st Qu.:2.0     1st Qu.:1.00   
 Median : 5.000   Median : 7.000   Median :9.0     Median :2.00   
 Mean   : 7.313   Mean   : 6.862   Mean   :6.2     Mean   :2.04   
 3rd Qu.: 6.000   3rd Qu.:10.000   3rd Qu.:9.0     3rd Qu.:2.00   
 Max.   :99.000   Max.   :99.000   Max.   :9.0     Max.   :9.00   
 NA&amp;#39;s   :13430    NA&amp;#39;s   :13434    NA&amp;#39;s   :13588   NA&amp;#39;s   :13568  
  birthwgt_lb3    birthwgt_oz3      lobthwgt3        cmbabdob   
 Min.   : 0.00   Min.   : 0.000   Min.   :8.00    Min.   : 833  
 1st Qu.: 6.00   1st Qu.: 0.000   1st Qu.:8.25    1st Qu.:1076  
 Median : 6.00   Median : 3.500   Median :8.50    Median :1134  
 Mean   :13.28   Mean   : 8.125   Mean   :8.50    Mean   :1140  
 3rd Qu.: 8.00   3rd Qu.: 9.000   3rd Qu.:8.75    3rd Qu.:1185  
 Max.   :99.00   Max.   :98.000   Max.   :9.00    Max.   :9999  
 NA&amp;#39;s   :13568   NA&amp;#39;s   :13569    NA&amp;#39;s   :13591   NA&amp;#39;s   :4445  
     kidage          hpagelb         birthplc       paybirth1    
 Min.   :   0.0   Min.   : 6.00   Min.   :1.000   Min.   :1.000  
 1st Qu.:  48.0   1st Qu.:23.00   1st Qu.:1.000   1st Qu.:1.000  
 Median :  99.0   Median :27.00   Median :1.000   Median :1.000  
 Mean   : 111.9   Mean   :28.73   Mean   :1.038   Mean   :1.924  
 3rd Qu.: 156.0   3rd Qu.:32.00   3rd Qu.:1.000   3rd Qu.:3.000  
 Max.   :9997.0   Max.   :99.00   Max.   :8.000   Max.   :9.000  
 NA&amp;#39;s   :4463     NA&amp;#39;s   :4445    NA&amp;#39;s   :10383   NA&amp;#39;s   :10383  
   paybirth2       paybirth3        knewpreg        trimestr    
 Min.   :1.000   Min.   :2       Min.   : 0.00   Min.   :1.000  
 1st Qu.:2.000   1st Qu.:3       1st Qu.: 3.00   1st Qu.:1.000  
 Median :2.000   Median :3       Median : 4.00   Median :1.000  
 Mean   :2.081   Mean   :3       Mean   : 6.01   Mean   :1.308  
 3rd Qu.:2.000   3rd Qu.:3       3rd Qu.: 6.00   3rd Qu.:2.000  
 Max.   :5.000   Max.   :5       Max.   :99.00   Max.   :2.000  
 NA&amp;#39;s   :12856   NA&amp;#39;s   :13584   NA&amp;#39;s   :9581    NA&amp;#39;s   :13580  
    ltrimest        priorsmk        postsmks        npostsmk    
 Min.   :1.00    Min.   :0.000   Min.   :1.000   Min.   :1.000  
 1st Qu.:1.25    1st Qu.:0.000   1st Qu.:5.000   1st Qu.:2.000  
 Median :1.50    Median :0.000   Median :5.000   Median :3.000  
 Mean   :1.50    Mean   :0.793   Mean   :4.435   Mean   :2.695  
 3rd Qu.:1.75    3rd Qu.:1.000   3rd Qu.:5.000   3rd Qu.:3.000  
 Max.   :2.00    Max.   :8.000   Max.   :9.000   Max.   :9.000  
 NA&amp;#39;s   :13591   NA&amp;#39;s   :9581    NA&amp;#39;s   :9581    NA&amp;#39;s   :13025  
    getprena        bgnprena         pnctrim         lpnctri     
 Min.   :1.000   Min.   : 1.000   Min.   :1.000   Min.   :1      
 1st Qu.:1.000   1st Qu.: 4.000   1st Qu.:1.000   1st Qu.:1      
 Median :1.000   Median : 6.000   Median :1.000   Median :1      
 Mean   :1.333   Mean   : 7.995   Mean   :1.455   Mean   :2      
 3rd Qu.:1.000   3rd Qu.: 9.000   3rd Qu.:2.000   3rd Qu.:1      
 Max.   :8.000   Max.   :99.000   Max.   :2.000   Max.   :9      
 NA&amp;#39;s   :9581    NA&amp;#39;s   :9914     NA&amp;#39;s   :13582   NA&amp;#39;s   :13584  
    workpreg        workborn        didwork         matweeks    
 Min.   :1.000   Min.   :1.000   Min.   :1.000   Min.   : 0.00  
 1st Qu.:1.000   1st Qu.:1.000   1st Qu.:2.000   1st Qu.: 6.00  
 Median :1.000   Median :1.000   Median :3.000   Median : 8.00  
 Mean   :2.703   Mean   :2.295   Mean   :2.358   Mean   :11.22  
 3rd Qu.:5.000   3rd Qu.:5.000   3rd Qu.:3.000   3rd Qu.:12.00  
 Max.   :9.000   Max.   :9.000   Max.   :7.000   Max.   :99.00  
 NA&amp;#39;s   :10407   NA&amp;#39;s   :11756   NA&amp;#39;s   :13000   NA&amp;#39;s   :12348  
    weeksdk         matleave        matchfound       livehere    
 Min.   :2.000   Min.   : 0.000   Min.   :1.000   Min.   :1.000  
 1st Qu.:2.000   1st Qu.: 0.000   1st Qu.:1.000   1st Qu.:1.000  
 Median :2.000   Median : 4.000   Median :1.000   Median :1.000  
 Mean   :4.375   Mean   : 5.341   Mean   :2.246   Mean   :2.105  
 3rd Qu.:7.500   3rd Qu.: 8.000   3rd Qu.:5.000   3rd Qu.:5.000  
 Max.   :9.000   Max.   :99.000   Max.   :7.000   Max.   :8.000  
 NA&amp;#39;s   :13585   NA&amp;#39;s   :12348    NA&amp;#39;s   :5390    NA&amp;#39;s   :11038  
    alivenow        cmkidied        cmkidlft        lastage      
 Min.   :1.000   Min.   :1002    Min.   : 964    Min.   :  0.00  
 1st Qu.:1.000   1st Qu.:1090    1st Qu.:1149    1st Qu.: 16.75  
 Median :1.000   Median :1158    Median :1197    Median : 66.00  
 Mean   :1.523   Mean   :1842    Mean   :1563    Mean   : 87.34  
 3rd Qu.:1.000   3rd Qu.:1196    3rd Qu.:1220    3rd Qu.:142.25  
 Max.   :8.000   Max.   :9999    Max.   :9999    Max.   :997.00  
 NA&amp;#39;s   :12890   NA&amp;#39;s   :13505   NA&amp;#39;s   :12977   NA&amp;#39;s   :12925   
    wherenow        legagree        parenend        anynurse    
 Min.   :1.000   Min.   :1.000   Min.   :1.000   Min.   :1.000  
 1st Qu.:1.000   1st Qu.:1.000   1st Qu.:1.000   1st Qu.:1.000  
 Median :2.000   Median :1.000   Median :1.000   Median :1.000  
 Mean   :2.282   Mean   :2.264   Mean   :1.366   Mean   :2.686  
 3rd Qu.:3.000   3rd Qu.:5.000   3rd Qu.:1.000   3rd Qu.:5.000  
 Max.   :9.000   Max.   :9.000   Max.   :9.000   Max.   :9.000  
 NA&amp;#39;s   :12977   NA&amp;#39;s   :13373   NA&amp;#39;s   :13060   NA&amp;#39;s   :5488   
    fedsolid       frsteatd_n        frsteatd_p       frsteatd      
 Min.   :1.000   Min.   :  0.000   Min.   :1.000   Min.   :  0.000  
 1st Qu.:1.000   1st Qu.:  2.000   1st Qu.:1.000   1st Qu.:  1.000  
 Median :1.000   Median :  3.000   Median :1.000   Median :  3.000  
 Mean   :1.679   Mean   :  6.798   Mean   :1.435   Mean   :  3.253  
 3rd Qu.:1.000   3rd Qu.:  6.000   3rd Qu.:2.000   3rd Qu.:  5.000  
 Max.   :9.000   Max.   :999.000   Max.   :7.000   Max.   :997.000  
 NA&amp;#39;s   :13166   NA&amp;#39;s   :8970      NA&amp;#39;s   :8982    NA&amp;#39;s   :8982     
    quitnurs       ageqtnur_n        ageqtnur_p       ageqtnur     
 Min.   :1.000   Min.   :  0.000   Min.   :1.000   Min.   :  0.00  
 1st Qu.:1.000   1st Qu.:  3.000   1st Qu.:1.000   1st Qu.:  2.00  
 Median :1.000   Median :  6.000   Median :1.000   Median :  5.00  
 Mean   :2.028   Mean   :  8.375   Mean   :1.217   Mean   :  6.87  
 3rd Qu.:5.000   3rd Qu.: 10.000   3rd Qu.:1.000   3rd Qu.: 10.00  
 Max.   :7.000   Max.   :999.000   Max.   :7.000   Max.   :997.00  
 NA&amp;#39;s   :12832   NA&amp;#39;s   :9164      NA&amp;#39;s   :9167    NA&amp;#39;s   :9167    
  matchfound2      livehere2       alivenow2       cmkidied2    
 Min.   :1.000   Min.   :1.000   Min.   :1.000   Min.   :1005   
 1st Qu.:1.000   1st Qu.:1.000   1st Qu.:1.000   1st Qu.:1084   
 Median :1.000   Median :1.000   Median :1.000   Median :1150   
 Mean   :2.796   Mean   :2.818   Mean   :2.467   Mean   :1127   
 3rd Qu.:5.000   3rd Qu.:5.000   3rd Qu.:5.000   3rd Qu.:1170   
 Max.   :5.000   Max.   :5.000   Max.   :5.000   Max.   :1225   
 NA&amp;#39;s   :13446   NA&amp;#39;s   :13527   NA&amp;#39;s   :13563   NA&amp;#39;s   :13582  
   cmkidlft2        lastage2        wherenow2       legagree2    
 Min.   :1069    Min.   :  0.00   Min.   :1.000   Min.   :1.000  
 1st Qu.:1150    1st Qu.:  0.00   1st Qu.:1.000   1st Qu.:1.000  
 Median :1197    Median :  5.00   Median :2.000   Median :1.000  
 Mean   :3030    Mean   : 41.68   Mean   :2.316   Mean   :2.714  
 3rd Qu.:1230    3rd Qu.: 60.00   3rd Qu.:2.000   3rd Qu.:5.000  
 Max.   :9999    Max.   :184.00   Max.   :6.000   Max.   :5.000  
 NA&amp;#39;s   :13574   NA&amp;#39;s   :13568    NA&amp;#39;s   :13574   NA&amp;#39;s   :13586  
   parenend2       anynurse2       fedsolid2      frsteatd_n2    
 Min.   :1.000   Min.   :1.00    Min.   :1.000   Min.   : 1.000  
 1st Qu.:1.000   1st Qu.:1.00    1st Qu.:1.000   1st Qu.: 1.000  
 Median :1.000   Median :1.00    Median :1.000   Median : 3.000  
 Mean   :1.667   Mean   :2.91    Mean   :1.286   Mean   : 3.071  
 3rd Qu.:1.000   3rd Qu.:5.00    3rd Qu.:1.000   3rd Qu.: 4.000  
 Max.   :5.000   Max.   :9.00    Max.   :5.000   Max.   :12.000  
 NA&amp;#39;s   :13575   NA&amp;#39;s   :13459   NA&amp;#39;s   :13579   NA&amp;#39;s   :13523   
  frsteatd_p2      frsteatd2        quitnurs2      ageqtnur_n2   
 Min.   :1.000   Min.   : 0.000   Min.   :1.0     Min.   : 1.00  
 1st Qu.:1.000   1st Qu.: 0.000   1st Qu.:1.0     1st Qu.: 2.50  
 Median :1.000   Median : 1.000   Median :1.0     Median : 4.00  
 Mean   :1.629   Mean   : 2.243   Mean   :1.6     Mean   : 5.94  
 3rd Qu.:2.000   3rd Qu.: 3.750   3rd Qu.:1.0     3rd Qu.: 8.00  
 Max.   :3.000   Max.   :12.000   Max.   :5.0     Max.   :40.00  
 NA&amp;#39;s   :13523   NA&amp;#39;s   :13523    NA&amp;#39;s   :13573   NA&amp;#39;s   :13526  
  ageqtnur_p2      ageqtnur2       matchfound3      livehere3    
 Min.   :1.000   Min.   : 0.000   Min.   :1.000   Min.   :1.000  
 1st Qu.:1.000   1st Qu.: 1.500   1st Qu.:1.000   1st Qu.:1.000  
 Median :1.000   Median : 3.000   Median :5.000   Median :5.000  
 Mean   :1.194   Mean   : 5.403   Mean   :3.316   Mean   :3.545  
 3rd Qu.:1.000   3rd Qu.: 7.500   3rd Qu.:5.000   3rd Qu.:5.000  
 Max.   :3.000   Max.   :40.000   Max.   :5.000   Max.   :5.000  
 NA&amp;#39;s   :13526   NA&amp;#39;s   :13526    NA&amp;#39;s   :13574   NA&amp;#39;s   :13582  
   alivenow3       cmkidied3       cmkidlft3        lastage3    
 Min.   :1.000   Min.   :1156    Min.   :1231    Min.   :  0.0  
 1st Qu.:1.000   1st Qu.:1158    1st Qu.:5615    1st Qu.:  0.0  
 Median :5.000   Median :1163    Median :9999    Median :  0.0  
 Mean   :3.286   Mean   :1177    Mean   :7076    Mean   : 24.4  
 3rd Qu.:5.000   3rd Qu.:1182    3rd Qu.:9999    3rd Qu.: 12.0  
 Max.   :5.000   Max.   :1225    Max.   :9999    Max.   :110.0  
 NA&amp;#39;s   :13586   NA&amp;#39;s   :13589   NA&amp;#39;s   :13590   NA&amp;#39;s   :13588  
   wherenow3       legagree3       parenend3       anynurse3    
 Min.   :1.000   Min.   :1       Min.   :1       Min.   :1.000  
 1st Qu.:3.500   1st Qu.:1       1st Qu.:1       1st Qu.:5.000  
 Median :6.000   Median :1       Median :1       Median :5.000  
 Mean   :5.333   Mean   :1       Mean   :1       Mean   :4.714  
 3rd Qu.:7.500   3rd Qu.:1       3rd Qu.:1       3rd Qu.:5.000  
 Max.   :9.000   Max.   :1       Max.   :1       Max.   :5.000  
 NA&amp;#39;s   :13590   NA&amp;#39;s   :13592   NA&amp;#39;s   :13590   NA&amp;#39;s   :13579  
 fedsolid3       frsteatd_n3     frsteatd_p3      frsteatd3    
 Mode:logical   Min.   :2       Min.   :1       Min.   :2      
 NA&amp;#39;s:13593     1st Qu.:2       1st Qu.:1       1st Qu.:2      
                Median :2       Median :1       Median :2      
                Mean   :2       Mean   :1       Mean   :2      
                3rd Qu.:2       3rd Qu.:1       3rd Qu.:2      
                Max.   :2       Max.   :1       Max.   :2      
                NA&amp;#39;s   :13592   NA&amp;#39;s   :13592   NA&amp;#39;s   :13592  
 quitnurs3       ageqtnur_n3     ageqtnur_p3      ageqtnur3    
 Mode:logical   Min.   :2       Min.   :1       Min.   :2      
 NA&amp;#39;s:13593     1st Qu.:2       1st Qu.:1       1st Qu.:2      
                Median :2       Median :1       Median :2      
                Mean   :2       Mean   :1       Mean   :2      
                3rd Qu.:2       3rd Qu.:1       3rd Qu.:2      
                Max.   :2       Max.   :1       Max.   :2      
                NA&amp;#39;s   :13592   NA&amp;#39;s   :13592   NA&amp;#39;s   :13592  
    cmlastlb       cmfstprg       cmlstprg       cmintstr   
 Min.   : 893   Min.   : 833   Min.   : 893   Min.   : 762  
 1st Qu.:1119   1st Qu.:1015   1st Qu.:1131   1st Qu.:1020  
 Median :1170   Median :1085   Median :1182   Median :1090  
 Mean   :1166   Mean   :1091   Mean   :1216   Mean   :1221  
 3rd Qu.:1206   3rd Qu.:1141   3rd Qu.:1213   3rd Qu.:1149  
 Max.   :9999   Max.   :9999   Max.   :9999   Max.   :9999  
 NA&amp;#39;s   :955    NA&amp;#39;s   :90     NA&amp;#39;s   :90     NA&amp;#39;s   :9     
    cmintfin      cmintstrop     cmintfinop     cmintstrcr   
 Min.   : 833   Min.   : 893   Min.   :1225   Min.   : 919   
 1st Qu.:1071   1st Qu.:1123   1st Qu.:1231   1st Qu.:1169   
 Median :1133   Median :1178   Median :1233   Median :1200   
 Mean   :1213   Mean   :1219   Mean   :1233   Mean   :1258   
 3rd Qu.:1185   3rd Qu.:1213   3rd Qu.:1235   3rd Qu.:1217   
 Max.   :9999   Max.   :9999   Max.   :1239   Max.   :9999   
 NA&amp;#39;s   :399    NA&amp;#39;s   :8950   NA&amp;#39;s   :8950   NA&amp;#39;s   :13243  
   cmintfincr       evuseint        stopduse       whystopd    
 Min.   :1227    Min.   :1.000   Min.   :1.00   Min.   :1.000  
 1st Qu.:1231    1st Qu.:1.000   1st Qu.:1.00   1st Qu.:1.000  
 Median :1233    Median :1.000   Median :1.00   Median :1.000  
 Mean   :1233    Mean   :2.157   Mean   :2.71   Mean   :2.519  
 3rd Qu.:1235    3rd Qu.:5.000   3rd Qu.:5.00   3rd Qu.:5.000  
 Max.   :1239    Max.   :9.000   Max.   :9.00   Max.   :9.000  
 NA&amp;#39;s   :13241   NA&amp;#39;s   :3029    NA&amp;#39;s   :5759   NA&amp;#39;s   :9090   
   whatmeth01       whatmeth02       whatmeth03      whatmeth04   
 Min.   : 1.000   Min.   : 1.000   Min.   : 3.00   Min.   : 7.00  
 1st Qu.: 3.000   1st Qu.: 4.000   1st Qu.: 7.00   1st Qu.: 7.00  
 Median : 4.000   Median : 7.000   Median :10.00   Median :13.00  
 Mean   : 5.044   Mean   : 7.637   Mean   :10.81   Mean   :12.07  
 3rd Qu.: 4.000   3rd Qu.:10.000   3rd Qu.:15.00   3rd Qu.:15.00  
 Max.   :99.000   Max.   :21.000   Max.   :21.00   Max.   :21.00  
 NA&amp;#39;s   :9432     NA&amp;#39;s   :12851    NA&amp;#39;s   :13465   NA&amp;#39;s   :13578  
    resnouse        wantbold        probbabe        cnfrmno     
 Min.   :1.000   Min.   :1.000   Min.   :1.000   Min.   :1.000  
 1st Qu.:1.000   1st Qu.:1.000   1st Qu.:1.000   1st Qu.:1.000  
 Median :5.000   Median :1.000   Median :1.000   Median :1.000  
 Mean   :3.144   Mean   :2.423   Mean   :3.253   Mean   :1.042  
 3rd Qu.:5.000   3rd Qu.:5.000   3rd Qu.:5.000   3rd Qu.:1.000  
 Max.   :9.000   Max.   :9.000   Max.   :9.000   Max.   :2.000  
 NA&amp;#39;s   :8664    NA&amp;#39;s   :5150    NA&amp;#39;s   :13344   NA&amp;#39;s   :13545  
    wantbld2        timingok       toosoon_n       toosoon_p    
 Min.   :1       Min.   :1.000   Min.   :  1.0   Min.   :1.000  
 1st Qu.:1       1st Qu.:1.000   1st Qu.:  2.0   1st Qu.:2.000  
 Median :1       Median :2.000   Median :  4.0   Median :2.000  
 Mean   :1       Mean   :1.815   Mean   : 38.2   Mean   :2.052  
 3rd Qu.:1       3rd Qu.:2.000   3rd Qu.:  6.0   3rd Qu.:2.000  
 Max.   :1       Max.   :9.000   Max.   :999.0   Max.   :9.000  
 NA&amp;#39;s   :13591   NA&amp;#39;s   :2757    NA&amp;#39;s   :9704    NA&amp;#39;s   :9704   
    wthpart1        wthpart2        feelinpg         hpwnold     
 Min.   :1.000   Min.   :1.000   Min.   : 1.000   Min.   :1.000  
 1st Qu.:1.000   1st Qu.:1.000   1st Qu.: 5.000   1st Qu.:1.000  
 Median :1.000   Median :2.000   Median : 9.000   Median :1.000  
 Mean   :1.225   Mean   :2.513   Mean   : 8.025   Mean   :2.161  
 3rd Qu.:1.000   3rd Qu.:4.000   3rd Qu.:10.000   3rd Qu.:5.000  
 Max.   :9.000   Max.   :9.000   Max.   :99.000   Max.   :9.000  
 NA&amp;#39;s   :6938    NA&amp;#39;s   :6655    NA&amp;#39;s   :10154                   
    timokhp         cohpbeg         cohpend         tellfath    
 Min.   :1.000   Min.   :1.000   Min.   :1.000   Min.   :1.000  
 1st Qu.:1.000   1st Qu.:1.000   1st Qu.:1.000   1st Qu.:1.000  
 Median :2.000   Median :1.000   Median :1.000   Median :1.000  
 Mean   :1.879   Mean   :2.852   Mean   :2.968   Mean   :1.271  
 3rd Qu.:2.000   3rd Qu.:5.000   3rd Qu.:5.000   3rd Qu.:1.000  
 Max.   :9.000   Max.   :9.000   Max.   :9.000   Max.   :9.000  
 NA&amp;#39;s   :3683    NA&amp;#39;s   :4615    NA&amp;#39;s   :4750    NA&amp;#39;s   :3333   
    whentell        tryscale         wantscal         whyprg1     
 Min.   :1.000   Min.   : 0.000   Min.   : 0.000   Min.   :1.000  
 1st Qu.:1.000   1st Qu.: 1.000   1st Qu.: 0.000   1st Qu.:1.000  
 Median :1.000   Median : 5.000   Median : 5.000   Median :2.000  
 Mean   :1.029   Mean   : 5.953   Mean   : 6.258   Mean   :1.792  
 3rd Qu.:1.000   3rd Qu.:10.000   3rd Qu.:10.000   3rd Qu.:2.000  
 Max.   :9.000   Max.   :99.000   Max.   :99.000   Max.   :9.000  
 NA&amp;#39;s   :4315    NA&amp;#39;s   :10234    NA&amp;#39;s   :10234    NA&amp;#39;s   :12804  
    whyprg2        whynouse1       whynouse2       whynouse3    
 Min.   :2       Min.   :1.000   Min.   :1.0     Min.   :1.000  
 1st Qu.:2       1st Qu.:2.000   1st Qu.:2.0     1st Qu.:5.000  
 Median :2       Median :2.000   Median :3.0     Median :6.000  
 Mean   :2       Mean   :2.884   Mean   :3.5     Mean   :4.778  
 3rd Qu.:2       3rd Qu.:3.000   3rd Qu.:5.0     3rd Qu.:6.000  
 Max.   :2       Max.   :9.000   Max.   :7.0     Max.   :6.000  
 NA&amp;#39;s   :13578   NA&amp;#39;s   :12886   NA&amp;#39;s   :13517   NA&amp;#39;s   :13584  
    anyusint       prglngth        outcome         birthord     
 Min.   :1.00   Min.   : 0.00   Min.   :1.000   Min.   : 1.000  
 1st Qu.:5.00   1st Qu.:13.00   1st Qu.:1.000   1st Qu.: 1.000  
 Median :5.00   Median :39.00   Median :1.000   Median : 2.000  
 Mean   :4.37   Mean   :29.53   Mean   :1.764   Mean   : 1.826  
 3rd Qu.:5.00   3rd Qu.:39.00   3rd Qu.:2.000   3rd Qu.: 2.000  
 Max.   :5.00   Max.   :50.00   Max.   :6.000   Max.   :10.000  
                                                NA&amp;#39;s   :4445    
     datend        agepreg        datecon         agecon    
 Min.   : 833   Min.   :1033   Min.   : 824   Min.   : 991  
 1st Qu.:1071   1st Qu.:2008   1st Qu.:1065   1st Qu.:1958  
 Median :1132   Median :2391   Median :1128   Median :2341  
 Mean   :1119   Mean   :2469   Mean   :1116   Mean   :2421  
 3rd Qu.:1184   3rd Qu.:2866   3rd Qu.:1181   3rd Qu.:2825  
 Max.   :1238   Max.   :4408   Max.   :1238   Max.   :4408  
 NA&amp;#39;s   :352    NA&amp;#39;s   :352                                 
    fmarout5        pmarpreg        rmarout6        fmarcon5    
 Min.   :1.000   Min.   :1.000   Min.   :1.000   Min.   :1.000  
 1st Qu.:1.000   1st Qu.:1.000   1st Qu.:1.000   1st Qu.:1.000  
 Median :2.000   Median :2.000   Median :2.000   Median :4.000  
 Mean   :2.876   Mean   :1.558   Mean   :3.256   Mean   :3.048  
 3rd Qu.:5.000   3rd Qu.:2.000   3rd Qu.:6.000   3rd Qu.:5.000  
 Max.   :5.000   Max.   :2.000   Max.   :6.000   Max.   :5.000  
 NA&amp;#39;s   :353     NA&amp;#39;s   :352     NA&amp;#39;s   :352                    
    learnprg         pncarewk        paydeliv          lbw1      
 Min.   : 0.000   Min.   : 1.00   Min.   :1.000   Min.   :1.000  
 1st Qu.: 3.000   1st Qu.: 4.00   1st Qu.:2.000   1st Qu.:2.000  
 Median : 4.000   Median : 7.00   Median :3.000   Median :2.000  
 Mean   : 5.297   Mean   :15.06   Mean   :3.078   Mean   :1.915  
 3rd Qu.: 6.000   3rd Qu.:11.00   3rd Qu.:4.000   3rd Qu.:2.000  
 Max.   :39.000   Max.   :95.00   Max.   :5.000   Max.   :2.000  
 NA&amp;#39;s   :9541     NA&amp;#39;s   :9541    NA&amp;#39;s   :10383   NA&amp;#39;s   :4445   
    bfeedwks        maternlv        oldwantr        oldwantp    
 Min.   :  0.0   Min.   :0.000   Min.   :1.000   Min.   :1.000  
 1st Qu.: 17.0   1st Qu.:0.000   1st Qu.:2.000   1st Qu.:2.000  
 Median : 61.0   Median :1.000   Median :3.000   Median :2.000  
 Mean   :465.2   Mean   :1.011   Mean   :2.867   Mean   :3.038  
 3rd Qu.:995.0   3rd Qu.:1.000   3rd Qu.:3.000   3rd Qu.:5.000  
 Max.   :995.0   Max.   :4.000   Max.   :6.000   Max.   :6.000  
 NA&amp;#39;s   :5595    NA&amp;#39;s   :10402                                  
    wantresp        wantpart        cmbirth          ager      
 Min.   :1.000   Min.   :1.000   Min.   : 688   Min.   :15.00  
 1st Qu.:2.000   1st Qu.:2.000   1st Qu.: 759   1st Qu.:28.00  
 Median :3.000   Median :2.000   Median : 819   Median :34.00  
 Mean   :2.867   Mean   :3.032   Mean   : 825   Mean   :33.47  
 3rd Qu.:3.000   3rd Qu.:5.000   3rd Qu.: 884   3rd Qu.:39.00  
 Max.   :6.000   Max.   :6.000   Max.   :1050   Max.   :44.00  
                                                               
    agescrn         fmarital        rmarital         educat     
 Min.   :15.00   Min.   :1.000   Min.   :1.000   Min.   : 9.00  
 1st Qu.:28.00   1st Qu.:1.000   1st Qu.:1.000   1st Qu.:11.00  
 Median :34.00   Median :1.000   Median :1.000   Median :12.00  
 Mean   :33.47   Mean   :2.496   Mean   :2.601   Mean   :12.66  
 3rd Qu.:39.00   3rd Qu.:5.000   3rd Qu.:4.000   3rd Qu.:14.00  
 Max.   :44.00   Max.   :5.000   Max.   :6.000   Max.   :19.00  
                                                                
     hieduc            race          hispanic        hisprace    
 Min.   : 5.000   Min.   :1.000   Min.   :1.000   Min.   :1.000  
 1st Qu.: 9.000   1st Qu.:1.000   1st Qu.:2.000   1st Qu.:2.000  
 Median : 9.000   Median :2.000   Median :2.000   Median :2.000  
 Mean   : 9.238   Mean   :1.813   Mean   :1.757   Mean   :2.086  
 3rd Qu.:10.000   3rd Qu.:2.000   3rd Qu.:2.000   3rd Qu.:3.000  
 Max.   :15.000   Max.   :3.000   Max.   :2.000   Max.   :4.000  
                                                                 
    rcurpreg       pregnum           parity          insuranc    
 Min.   :1.00   Min.   : 1.000   Min.   : 0.000   Min.   :1.000  
 1st Qu.:2.00   1st Qu.: 2.000   1st Qu.: 1.000   1st Qu.:2.000  
 Median :2.00   Median : 3.000   Median : 2.000   Median :2.000  
 Mean   :1.93   Mean   : 3.698   Mean   : 2.402   Mean   :2.142  
 3rd Qu.:2.00   3rd Qu.: 5.000   3rd Qu.: 3.000   3rd Qu.:3.000  
 Max.   :2.00   Max.   :19.000   Max.   :22.000   Max.   :4.000  
                                                                 
    pubassis       poverty         laborfor        religion    
 Min.   :1.00   Min.   :  7.0   Min.   :1.000   Min.   :1.000  
 1st Qu.:1.00   1st Qu.: 96.0   1st Qu.:1.000   1st Qu.:2.000  
 Median :2.00   Median :180.0   Median :2.000   Median :3.000  
 Mean   :1.61   Mean   :219.2   Mean   :3.436   Mean   :2.491  
 3rd Qu.:2.00   3rd Qu.:328.0   3rd Qu.:7.000   3rd Qu.:3.000  
 Max.   :2.00   Max.   :500.0   Max.   :9.000   Max.   :4.000  
                                                               
     metro           brnout         yrstrus        prglngth_i     
 Min.   :1.000   Min.   :1.000   Min.   :1959    Min.   :0.00000  
 1st Qu.:1.000   1st Qu.:5.000   1st Qu.:1981    1st Qu.:0.00000  
 Median :2.000   Median :5.000   Median :1990    Median :0.00000  
 Mean   :1.665   Mean   :4.259   Mean   :2135    Mean   :0.01832  
 3rd Qu.:2.000   3rd Qu.:5.000   3rd Qu.:1996    3rd Qu.:0.00000  
 Max.   :3.000   Max.   :9.000   Max.   :9999    Max.   :2.00000  
                                 NA&amp;#39;s   :11036                    
   outcome_i          birthord_i          datend_i         agepreg_i      
 Min.   :0.000000   Min.   :0.000000   Min.   :0.00000   Min.   :0.00000  
 1st Qu.:0.000000   1st Qu.:0.000000   1st Qu.:0.00000   1st Qu.:0.00000  
 Median :0.000000   Median :0.000000   Median :0.00000   Median :0.00000  
 Mean   :0.005591   Mean   :0.005591   Mean   :0.01515   Mean   :0.01847  
 3rd Qu.:0.000000   3rd Qu.:0.000000   3rd Qu.:0.00000   3rd Qu.:0.00000  
 Max.   :2.000000   Max.   :2.000000   Max.   :2.00000   Max.   :2.00000  
                                                                          
   datecon_i         agecon_i         fmarout5_i        pmarpreg_i     
 Min.   :0.0000   Min.   :0.00000   Min.   :0.00000   Min.   :0.00000  
 1st Qu.:0.0000   1st Qu.:0.00000   1st Qu.:0.00000   1st Qu.:0.00000  
 Median :0.0000   Median :0.00000   Median :0.00000   Median :0.00000  
 Mean   :0.0156   Mean   :0.01589   Mean   :0.08034   Mean   :0.03517  
 3rd Qu.:0.0000   3rd Qu.:0.00000   3rd Qu.:0.00000   3rd Qu.:0.00000  
 Max.   :2.0000   Max.   :1.00000   Max.   :1.00000   Max.   :2.00000  
                                                                       
   rmarout6_i        fmarcon5_i        learnprg_i        pncarewk_i     
 Min.   :0.00000   Min.   :0.00000   Min.   :0.00000   Min.   :0.00000  
 1st Qu.:0.00000   1st Qu.:0.00000   1st Qu.:0.00000   1st Qu.:0.00000  
 Median :0.00000   Median :0.00000   Median :0.00000   Median :0.00000  
 Mean   :0.07533   Mean   :0.07555   Mean   :0.01876   Mean   :0.01118  
 3rd Qu.:0.00000   3rd Qu.:0.00000   3rd Qu.:0.00000   3rd Qu.:0.00000  
 Max.   :1.00000   Max.   :2.00000   Max.   :2.00000   Max.   :2.00000  
                                                                        
   paydeliv_i           lbw1_i           bfeedwks_i     
 Min.   :0.000000   Min.   :0.000000   Min.   :0.00000  
 1st Qu.:0.000000   1st Qu.:0.000000   1st Qu.:0.00000  
 Median :0.000000   Median :0.000000   Median :0.00000  
 Mean   :0.005812   Mean   :0.006621   Mean   :0.00721  
 3rd Qu.:0.000000   3rd Qu.:0.000000   3rd Qu.:0.00000  
 Max.   :2.000000   Max.   :2.000000   Max.   :2.00000  
                                                        
   maternlv_i         oldwantr_i        oldwantp_i         wantresp_i     
 Min.   :0.000000   Min.   :0.00000   Min.   :0.000000   Min.   :0.00000  
 1st Qu.:0.000000   1st Qu.:0.00000   1st Qu.:0.000000   1st Qu.:0.00000  
 Median :0.000000   Median :0.00000   Median :0.000000   Median :0.00000  
 Mean   :0.006915   Mean   :0.01412   Mean   :0.005812   Mean   :0.01412  
 3rd Qu.:0.000000   3rd Qu.:0.00000   3rd Qu.:0.000000   3rd Qu.:0.00000  
 Max.   :2.000000   Max.   :1.00000   Max.   :1.000000   Max.   :1.00000  
                                                                          
   wantpart_i           ager_i    fmarital_i   rmarital_i
 Min.   :0.000000   Min.   :0   Min.   :0    Min.   :0   
 1st Qu.:0.000000   1st Qu.:0   1st Qu.:0    1st Qu.:0   
 Median :0.000000   Median :0   Median :0    Median :0   
 Mean   :0.007945   Mean   :0   Mean   :0    Mean   :0   
 3rd Qu.:0.000000   3rd Qu.:0   3rd Qu.:0    3rd Qu.:0   
 Max.   :1.000000   Max.   :0   Max.   :0    Max.   :0   
                                                         
    educat_i            hieduc_i           race_i    hispanic_i       
 Min.   :0.0000000   Min.   :0.00000   Min.   :0   Min.   :0.0000000  
 1st Qu.:0.0000000   1st Qu.:0.00000   1st Qu.:0   1st Qu.:0.0000000  
 Median :0.0000000   Median :0.00000   Median :0   Median :0.0000000  
 Mean   :0.0001471   Mean   :0.00103   Mean   :0   Mean   :0.0003678  
 3rd Qu.:0.0000000   3rd Qu.:0.00000   3rd Qu.:0   3rd Qu.:0.0000000  
 Max.   :1.0000000   Max.   :2.00000   Max.   :0   Max.   :1.0000000  
                                                                      
   hisprace_i   rcurpreg_i   pregnum_i    parity_i      
 Min.   :0    Min.   :0    Min.   :0   Min.   :0.00000  
 1st Qu.:0    1st Qu.:0    1st Qu.:0   1st Qu.:0.00000  
 Median :0    Median :0    Median :0   Median :0.00000  
 Mean   :0    Mean   :0    Mean   :0   Mean   :0.02634  
 3rd Qu.:0    3rd Qu.:0    3rd Qu.:0   3rd Qu.:0.00000  
 Max.   :0    Max.   :0    Max.   :0   Max.   :2.00000  
                                                        
   insuranc_i         pubassis_i        poverty_i      
 Min.   :0.000000   Min.   :0.00000   Min.   :0.00000  
 1st Qu.:0.000000   1st Qu.:0.00000   1st Qu.:0.00000  
 Median :0.000000   Median :0.00000   Median :0.00000  
 Mean   :0.003458   Mean   :0.01221   Mean   :0.05341  
 3rd Qu.:0.000000   3rd Qu.:0.00000   3rd Qu.:0.00000  
 Max.   :2.000000   Max.   :1.00000   Max.   :1.00000  
                                                       
   laborfor_i          religion_i          metro_i     basewgt        
 Min.   :0.0000000   Min.   :0.000000   Min.   :0   Min.   :   64.58  
 1st Qu.:0.0000000   1st Qu.:0.000000   1st Qu.:0   1st Qu.: 2335.45  
 Median :0.0000000   Median :0.000000   Median :0   Median : 3409.65  
 Mean   :0.0008092   Mean   :0.003016   Mean   :0   Mean   : 4216.27  
 3rd Qu.:0.0000000   3rd Qu.:0.000000   3rd Qu.:0   3rd Qu.: 4869.94  
 Max.   :1.0000000   Max.   :2.000000   Max.   :0   Max.   :99707.83  
                                                                      
 adj_mod_basewgt       finalwgt            secu_p           sest      
 Min.   :    71.2   Min.   :   118.7   Min.   :1.000   Min.   : 1.00  
 1st Qu.:  2798.1   1st Qu.:  3841.4   1st Qu.:1.000   1st Qu.:25.00  
 Median :  4127.2   Median :  6256.6   Median :1.000   Median :45.00  
 Mean   :  5384.0   Mean   :  8196.4   Mean   :1.487   Mean   :44.08  
 3rd Qu.:  5795.7   3rd Qu.:  9432.4   3rd Qu.:2.000   3rd Qu.:65.00  
 Max.   :157143.7   Max.   :261880.0   Max.   :2.000   Max.   :84.00  
                                                                      
    cmintvw    
 Min.   :1225  
 1st Qu.:1231  
 Median :1233  
 Mean   :1233  
 3rd Qu.:1235  
 Max.   :1239  
               &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see a lot of missing values. We’ll clean the data for the columns that we want to analyze.&lt;/p&gt;
&lt;div id=&#34;transformation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Transformation&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;agepreg contains the mother’s age at the end of the pregnancy. In the data file, agepreg is encoded as an integer number of centiyears. So the first line divides each element of agepreg by 100, yielding a floating-point value in years.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;birthwgt_lb and birthwgt_oz contain the weight of the baby, in pounds and ounces, for pregnancies that end in live birth. In addition it uses several special codes:  97 NOT ASCERTAINED 98 REFUSED 99 DONT KNOW Special values encoded as numbers are dangerous because if they are not handled properly, they can generate bogus results, like a 99-pound baby. Assuming that a baby can’t be generally more than 20 lb at birth, we will replace all other values with NA, as they are NOT ASCERTAINED(97), REFUSED(98), DONT KNOW(99), or invalid values. Similarly, the age of father has these similar special codes, which we will replace by NA&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cleanFemPreg &amp;lt;- function(data){
  # mother&amp;#39;s age is encoded in centiyears; convert to years
  data[&amp;#39;agepreg&amp;#39;] &amp;lt;-  data[&amp;#39;agepreg&amp;#39;]/100.0
  
  # birthwgt_lb contains at least one bogus value (51 lbs)
  # replace with NaN
  data$birthwgt_lb[data$birthwgt_lb &amp;gt; 20] &amp;lt;- NA
  
  # replace &amp;#39;not ascertained&amp;#39;, &amp;#39;refused&amp;#39;, &amp;#39;don&amp;#39;t know&amp;#39; with NA
  na_vals = c(97, 98, 99)
  data$birthwgt_oz[data$birthwgt_oz %in% na_vals] &amp;lt;- NA
  data$hpagelb[data$hpagelb %in% na_vals] &amp;lt;- NA
  
  # birthweight is stored in two columns, lbs and oz.
  # convert to a single column in lb
  data[&amp;#39;totalwgt_lb&amp;#39;] &amp;lt;- data$birthwgt_lb + (data$birthwgt_oz / 16.0)
  
  return (data)
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;femPregCleaned &amp;lt;- cleanFemPreg(femPreg2002)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;validation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Validation&lt;/h3&gt;
&lt;p&gt;One way to validate data is to compute basic statistics and compare them with published results. For example, the NSFG codebook includes tables that summarize each variable. Here is the table for outcome, which encodes the outcome of each pregnancy: value label Total 1 LIVE BIRTH 9148 2 INDUCED ABORTION 1862 3 STILLBIRTH 120 4 MISCARRIAGE 1921 5 ECTOPIC PREGNANCY 190 6 CURRENT PREGNANCY 352&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;femPreg2002 %&amp;gt;%
  group_by(outcome) %&amp;gt;%
  summarise(Total = length(outcome))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 6 x 2
  outcome Total
    &amp;lt;int&amp;gt; &amp;lt;int&amp;gt;
1       1  9148
2       2  1862
3       3   120
4       4  1921
5       5   190
6       6   352&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Comparing the results with the published table, it looks like the values in outcome are correct. Similarly, here is the published table for birthwgt_lb value label Total . INAPPLICABLE 4449 0-5 UNDER 6 POUNDS 1125 6 6 POUNDS 2223 7 7 POUNDS 3049 8 8 POUNDS 1889 9-95 9 POUNDS OR MORE 799&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;femPreg2002 %&amp;gt;%
  group_by(birthwgt_lb) %&amp;gt;%
  summarise(Total = length(birthwgt_lb))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 21 x 2
   birthwgt_lb Total
         &amp;lt;int&amp;gt; &amp;lt;int&amp;gt;
 1           0     8
 2           1    40
 3           2    53
 4           3    98
 5           4   229
 6           5   697
 7           6  2223
 8           7  3049
 9           8  1889
10           9   623
# ... with 11 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The counts for 6, 7, and 8 pounds check out, and if you add up the counts for 0-5 and 9-95, they check out, too. But if you look more closely, you will notice one value that has to be an error, a 51 pound baby! This has been cleaned in the cleanFemPreg function.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;interpretation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Interpretation&lt;/h3&gt;
&lt;p&gt;To work with data effectively, you have to think on two levels at the same time: the level of statistics and the level of context. As an example, let’s look at the sequence of outcomes for a few respondents. This example looks up one respondent and prints a list of outcomes for her pregnancies:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;CASEID = 10229
femPregCleaned %&amp;gt;%
  filter(caseid==CASEID) %&amp;gt;%
  .$outcome&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 4 4 4 4 4 4 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The outcome code 1 indicates a live birth. Code 4 indicates a miscarriage; that is, a pregnancy that ended spontaneously, usually with no known medical cause.&lt;/p&gt;
&lt;p&gt;Statistically this respondent is not unusual. Miscarriages are common and there are other respondents who reported as many or more. But remembering the context, this data tells the story of a woman who was pregnant six times, each time ending in miscarriage. Her seventh and most recent pregnancy ended in a live birth. If we consider this data with empathy, it is natural to be moved by the story it tells.&lt;/p&gt;
&lt;p&gt;Each record in the NSFG dataset represents a person who provided honest answers to many personal and difficult questions. We can use this data to answer statistical questions about family life, reproduction, and health. At the same time, we have an obligation to consider the people represented by the data, and to afford them respect and gratitude.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Example Talk</title>
      <link>/talk/example-talk/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 -0500</pubDate>
      
      <guid>/talk/example-talk/</guid>
      <description>&lt;p&gt;Embed your slides or video here using &lt;a href=&#34;https://sourcethemes.com/academic/post/writing-markdown-latex/&#34; target=&#34;_blank&#34;&gt;shortcodes&lt;/a&gt;. Further details can easily be added using &lt;em&gt;Markdown&lt;/em&gt; and $\rm \LaTeX$ math code.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
