<!DOCTYPE html>
<html lang="en-us">
<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="theme" content="hugo-academic">
  <meta name="generator" content="Hugo 0.37" />
  <meta name="author" content="Kumar Rohit Malhotra">

  
  
  
  
    
      
    
  
  <meta name="description" content="Regression analysis is a powerful statistical process to find the relations within a dataset, with the key focus being on relationships between the independent variables (predictors) and a dependent variable (outcome). It can be used to build models for inference or prediction. Among several methods of regression analysis, linear regression sets the basis and is quite widely used for several real-world applications.
Source : Google Images
 In this post, we will look at building a linear regression model for inference.">

  
  <link rel="alternate" hreflang="en-us" href="/post/linear-regression-modeling-and-assumptions/">

  


  

  
  
  <meta name="theme-color" content="#0095eb">
  
  
  
  
    
  
  
    
    
      
        <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">
      
    
  
  
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha512-6MXa8B6uaO18Hid6blRMetEIoPqHf7Ux1tnyIQdpt9qI5OACx7C+O3IVTr98vwGnlcg0LOLa02i9Y1HpVhlfiw==" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.1/css/academicons.min.css" integrity="sha512-NThgw3XKQ1absAahW6to7Ey42uycrVvfNfyjqcFNgCmOCQ5AR4AO0SiXrN+8ZtYeappp56lk1WtvjVmEa+VR6A==" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha512-SfTiTlX6kk+qitfevl/7LibUOeJWlt9rbyDn92a1DqWOw9vWG2MFoays0sgObmWazO5BQPiFucnnEAjpAB+/Sw==" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">
  
  
  
  
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Montserrat:400,700%7cRoboto:400,400italic,700%7cRoboto&#43;Mono">
  
  <link rel="stylesheet" href="/styles.css">
  

  
    <script>
      window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
      ga('create', 'UA-117115991-1', 'auto');
      ga('require', 'eventTracker');
      ga('require', 'outboundLinkTracker');
      ga('require', 'urlChangeTracker');
      ga('send', 'pageview');
    </script>
    <script async src="//www.google-analytics.com/analytics.js"></script>
    
    <script async src="https://cdnjs.cloudflare.com/ajax/libs/autotrack/2.4.1/autotrack.js" integrity="sha512-HUmooslVKj4m6OBu0OgzjXXr+QuFYy/k7eLI5jdeEy/F4RSgMn6XRWRGkFi5IFaFgy7uFTkegp3Z0XnJf3Jq+g==" crossorigin="anonymous"></script>
    
  

  
  <link rel="alternate" href="/index.xml" type="application/rss+xml" title="Did you say data?">
  <link rel="feed" href="/index.xml" type="application/rss+xml" title="Did you say data?">
  

  <link rel="manifest" href="/site.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="/post/linear-regression-modeling-and-assumptions/">

  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="twitter:site" content="@krohitm">
  <meta property="twitter:creator" content="@krohitm">
  
  <meta property="og:site_name" content="Did you say data?">
  <meta property="og:url" content="/post/linear-regression-modeling-and-assumptions/">
  <meta property="og:title" content="Linear regression: Modeling and Assumptions | Did you say data?">
  <meta property="og:description" content="Regression analysis is a powerful statistical process to find the relations within a dataset, with the key focus being on relationships between the independent variables (predictors) and a dependent variable (outcome). It can be used to build models for inference or prediction. Among several methods of regression analysis, linear regression sets the basis and is quite widely used for several real-world applications.
Source : Google Images
 In this post, we will look at building a linear regression model for inference.">
  <meta property="og:locale" content="en-us">
  
  <meta property="article:published_time" content="2018-09-09T00:00:00&#43;00:00">
  
  <meta property="article:modified_time" content="2018-09-09T00:00:00&#43;00:00">
  

  

  <title>Linear regression: Modeling and Assumptions | Did you say data?</title>

</head>
<body id="top" data-spy="scroll" data-target="#toc" data-offset="71" >

<nav class="navbar navbar-default navbar-fixed-top" id="navbar-main">
  <div class="container">

    
    <div class="navbar-header">
      
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse"
              data-target=".navbar-collapse" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      
      <a class="navbar-brand" href="/">Did you say data?</a>
    </div>

    
    <div class="collapse navbar-collapse">

      
      
      <ul class="nav navbar-nav navbar-right">
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#about">
            
            <span>Home</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#publications">
            
            <span>Publications</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#posts">
            
            <span>Posts</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#projects">
            
            <span>Projects</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#contact">
            
            <span>Contact</span>
            
          </a>
        </li>

        
        
      

      
      </ul>

    </div>
  </div>
</nav>


<article class="article" itemscope itemtype="http://schema.org/Article">

  


  <div class="article-container">
    <h1 itemprop="name">Linear regression: Modeling and Assumptions</h1>

    

<div class="article-metadata">

  <span class="article-date">
    
    <time datetime="2018-09-09 00:00:00 &#43;0000 UTC" itemprop="datePublished dateModified">
      Sep 9, 2018
    </time>
  </span>
  <span itemscope itemprop="author publisher" itemtype="http://schema.org/Person">
    <meta itemprop="name" content="Kumar Rohit Malhotra">
  </span>

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    20 min read
  </span>
  

  
  
  <span class="middot-divider"></span>
  <a href="/post/linear-regression-modeling-and-assumptions/#disqus_thread"></a>
  

  
  
  
  <span class="middot-divider"></span>
  <span class="article-categories">
    <i class="fa fa-folder"></i>
    
    <a href="/categories/data-analysis">data analysis</a
    >
    
  </span>
  
  

  
  
<div class="share-box" aria-hidden="true">
  <ul class="share">
    <li>
      <a class="twitter"
         href="https://twitter.com/intent/tweet?text=Linear%20regression%3a%20Modeling%20and%20Assumptions&amp;url=%2fpost%2flinear-regression-modeling-and-assumptions%2f"
         target="_blank" rel="noopener">
        <i class="fa fa-twitter"></i>
      </a>
    </li>
    <li>
      <a class="facebook"
         href="https://www.facebook.com/sharer.php?u=%2fpost%2flinear-regression-modeling-and-assumptions%2f"
         target="_blank" rel="noopener">
        <i class="fa fa-facebook"></i>
      </a>
    </li>
    <li>
      <a class="linkedin"
         href="https://www.linkedin.com/shareArticle?mini=true&amp;url=%2fpost%2flinear-regression-modeling-and-assumptions%2f&amp;title=Linear%20regression%3a%20Modeling%20and%20Assumptions"
         target="_blank" rel="noopener">
        <i class="fa fa-linkedin"></i>
      </a>
    </li>
    <li>
      <a class="weibo"
         href="http://service.weibo.com/share/share.php?url=%2fpost%2flinear-regression-modeling-and-assumptions%2f&amp;title=Linear%20regression%3a%20Modeling%20and%20Assumptions"
         target="_blank" rel="noopener">
        <i class="fa fa-weibo"></i>
      </a>
    </li>
    <li>
      <a class="email"
         href="mailto:?subject=Linear%20regression%3a%20Modeling%20and%20Assumptions&amp;body=%2fpost%2flinear-regression-modeling-and-assumptions%2f">
        <i class="fa fa-envelope"></i>
      </a>
    </li>
  </ul>
</div>


  

</div>


    <div class="article-style" itemprop="articleBody">
      <p>Regression analysis is a powerful statistical process to find the relations within a dataset, with the key focus being on relationships between the independent variables (predictors) and a dependent variable (outcome). It can be used to build models for inference or prediction. Among several methods of regression analysis, linear regression sets the basis and is quite widely used for <a href="https://en.wikipedia.org/wiki/Linear_regression#Applications" target="_blank">several real-world applications</a>.</p>
<div class="figure">
<img src="post/2018-09-05-linear-regression-modeling-and-assumptions_files/2120406.jpg" alt="Source : Google Images" />
<p class="caption">Source : Google Images</p>
</div>
<p>In this post, we will look at building a linear regression model for inference. The dataset we will use is the insurance charges data obtained from <a href="https://www.kaggle.com/mirichoi0218/insurance/home" target="_blank">Kaggle</a>. This data set consists of 1,338 observations and 7 columns: age, sex, bmi, children, smoker, region and charges.</p>
<p>The key questions that we would be asking are:</p>
<ol style="list-style-type: decimal">
<li>Is there a relationship between medical charges and other variables in the dataset?</li>
<li>How valid is the model we have built?</li>
<li>What can we do to improve the model?</li>
</ol>
<p>We start with importing the required libraries and data:</p>
<pre class="r"><code>library(magrittr)
library(purrr)
library(MASS)
library(car)
library(broom)
library(ggplot2)
library(psych)
library(caret)
library(tidyr)
library(effects)
library(tibble)</code></pre>
<pre class="r"><code>insurance &lt;- read.csv(&#39;~/Documents/CodeWork/medicalCost/insurance.csv&#39;)
summary(insurance)</code></pre>
<pre><code>      age            sex           bmi           children     smoker    
 Min.   :18.00   female:662   Min.   :15.96   Min.   :0.000   no :1064  
 1st Qu.:27.00   male  :676   1st Qu.:26.30   1st Qu.:0.000   yes: 274  
 Median :39.00                Median :30.40   Median :1.000             
 Mean   :39.21                Mean   :30.66   Mean   :1.095             
 3rd Qu.:51.00                3rd Qu.:34.69   3rd Qu.:2.000             
 Max.   :64.00                Max.   :53.13   Max.   :5.000             
       region       charges     
 northeast:324   Min.   : 1122  
 northwest:325   1st Qu.: 4740  
 southeast:364   Median : 9382  
 southwest:325   Mean   :13270  
                 3rd Qu.:16640  
                 Max.   :63770  </code></pre>
<p>Some simple observations that can be taken from the summary are:</p>
<ol style="list-style-type: decimal">
<li>The age of participants varies from 18 to 64.</li>
<li>Around 49.48% of participants are female.</li>
<li>The bmi of participants ranges from 15.96 to 53.13.</li>
<li>Only 20.48% of the participants are smokers.</li>
</ol>
<p>Let’s start with building a linear model. Instead of simple linear regression, where you have one predictor and one outcome, we will go with multiple linear regression, where you have more than one predictors and one outcome.</p>
<p>Multiple linear regression follows the formula :</p>
<p><span class="math display">\[y = beta_0{}+ beta_1x_1+beta_2x_2+...\]</span></p>
<p>The coefficients in this linear equation denote the magnitude of additive relation between the predictor and the response. In simpler words, keeping everything else fixed, a unit change in x_1 will lead to change of beta_1 in the outcome, and so on.</p>
<div id="is-there-a-relationship-between-the-medical-charges-and-the-predictors" class="section level2">
<h2>Is there a relationship between the medical charges and the predictors?</h2>
<p>Our first step is finding if there is any relationship between the outcome and the predictors.</p>
<p>The null hypothesis would be that there is no relation between any of the predictors and the response, which would be possible when all the regression coefficients for the predictors are equal to zero. The alternate hypothesis would be that atleast one of the predictors has a relation with the outcome, that is the regression coefficient of one of the predictors is non-zero.</p>
<p>This hypothesis is tested by computing the <a href="http://www.statisticshowto.com/probability-and-statistics/F%20statistic-value-test/" target="_blank">F statistic</a>. In case of no relationship between the predictor and the response, F statistic will be closer to 1. On the contrary, if the alternate hypothesis is true, the F statistic will be greater than 1. The p-value of F statistic can be used to determine whether the null hypothesis can be rejected or not.</p>
<p>We will start with fitting a multiple linear regression model using all the predictors:</p>
<pre class="r"><code>lm.fit &lt;- lm(formula = charges~., data = insurance)
#Here &#39;.&#39; means we are using all the predictors in the dataset.
summary(lm.fit)</code></pre>
<pre><code>
Call:
lm(formula = charges ~ ., data = insurance)

Residuals:
     Min       1Q   Median       3Q      Max 
-11304.9  -2848.1   -982.1   1393.9  29992.8 

Coefficients:
                Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)     -11938.5      987.8 -12.086  &lt; 2e-16 ***
age                256.9       11.9  21.587  &lt; 2e-16 ***
sexmale           -131.3      332.9  -0.394 0.693348    
bmi                339.2       28.6  11.860  &lt; 2e-16 ***
children           475.5      137.8   3.451 0.000577 ***
smokeryes        23848.5      413.1  57.723  &lt; 2e-16 ***
regionnorthwest   -353.0      476.3  -0.741 0.458769    
regionsoutheast  -1035.0      478.7  -2.162 0.030782 *  
regionsouthwest   -960.0      477.9  -2.009 0.044765 *  
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

Residual standard error: 6062 on 1329 degrees of freedom
Multiple R-squared:  0.7509,    Adjusted R-squared:  0.7494 
F-statistic: 500.8 on 8 and 1329 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>A high value of F statistic, with a very low p-value (&lt;2.2e-16), implies that the null hypothesis can be rejected. This means there is a potential relationship between the predictors and the outcome.</p>
<p>RSE (Residual Standard Error) is the estimate of standard deviation of irreducible error (the error which can’t be reduced even if knew the true regression line; hence, irreducible). In simpler words, it is the average deviation between the actual outcome and the true regression line. Hence, a large value of RSE means a high deviation from the true regression line. As such, RSE is useful in determining the lack of fit of the model to the data. RSE in our model is large (6062), indicating that the model doesn’t fit the data well.</p>
<p>R-squared measures the proportion of variability in the outcome that can be explained by the predictor, and is always between 0 and 1; the higher the value, the better the model is able to explain the variability in the outcome. However, increase in number of predictors mostly results in an increased value of R-squared due to <a href="https://en.wikipedia.org/wiki/Coefficient_of_determination#Inflation_of_R2" target="_blank">inflation of R-squared</a>. <a href="https://en.wikipedia.org/wiki/Coefficient_of_determination#Adjusted_R2" target="_blank">Adjusted R-squared</a> adjusts the value of R-squared to avoid this effect. A high value of adjusted R-squared (0.7494) shows that more than 74% of the variance in the data is being explained by the model. From now on, we will use the term R-squared for adjusted R-squared.</p>
<p>The Std. Error gives us the average amount that the estimated coefficient of a predictor differs from the actual coefficient of predictor. It can be used to compute the confidence interval of an estimated coefficient, which we will see later.</p>
<p>The <em>t value</em> of a predictor tells us how many standard deviations its estimated coefficient is away from 0. <em>Pr (&gt;|t|)</em> for a predictor is the p-value for the estimated regression coefficient, which is same as saying what is the probability of . A very low p-value (&lt;0.05) for a predictor can be used to infer that there is a relationsip between the predictor and the outcome.</p>
<p>Our next step should be <a href="https://en.wikipedia.org/wiki/Regression_validation" target="_blank">validation of regression analyis</a>. This may mean validation of underlying assumptions of the model, checking the structure of model with different predictors, looking for values that have an exceptionally large impact on the regression model, looking for observations that have not been represented well enough in the model, and more.</p>
<p>In the rest of the post, we will look at some of these methods of model validation and improvement.</p>
<hr />
</div>
<div id="which-variables-have-a-strong-relation-to-medical-charges" class="section level2">
<h2>Which variables have a strong relation to medical charges?</h2>
<p>Now that we have determined that there is a relation between the predictors and the outcome, our next step would be finding out if all or only some of the predictors are related to the outcome.</p>
<p>If we look at the p-values of the estimated coefficients above, we see that not all the coefficients are statistically significant (&lt;0.05). This means that only a subset of the predictors are related to the outcome.</p>
<p>We can look at the individual p-values for selecting the variables. This may not be a problem when the number of predictors (7) is quite small compared to the number of observations (1338). This method won’t, however, work when the number of predictors is greater than the number of observations because of the <a href="http://www.statisticshowto.com/multiple-testing-problem/" target="_blank">multiple testing problem</a>. In such cases, we would have to use the <a href="https://en.wikipedia.org/wiki/Feature_selection" target="_blank">feature/variable selection</a> methods, like forward selection, backward selection, or mixed selection.</p>
<div class="figure">
<img src="static/post/2018-09-05-linear-regression-modeling-and-assumptions_files/RegressionMeme.jpg" alt="Source: Google Images" />
<p class="caption">Source: Google Images</p>
</div>
<p>Before jumping on to feature selection using any of these methods, let us try linear regression using the features with significant p-values only.</p>
<pre class="r"><code>lm.fit.sel &lt;- lm(charges~age+bmi+children+smoker+region, data = insurance)</code></pre>
<p>We will compare this to <a href="https://www.stat.ubc.ca/~rollin/teach/643w04/lec/node43.html" target="_blank">mixed selection</a>, which is a combination of forward and backward selection. This can be done in R using the <em>stepAIC()</em> function, which uses <a href="https://en.wikipedia.org/wiki/Akaike_information_criterion" target="_blank">Akaike Information Criterion</a> (AIC) to select the best model out of multiple models.</p>
<pre class="r"><code>#selecting direction = &quot;both&quot; for mixed selection
step.lm.fit &lt;- stepAIC(lm.fit, direction = &quot;both&quot;, trace = FALSE)</code></pre>
<p>Let’s compare the two models :</p>
<pre class="r"><code>step.lm.fit$call</code></pre>
<pre><code>lm(formula = charges ~ age + bmi + children + smoker + region, 
    data = insurance)</code></pre>
<pre class="r"><code>lm.fit.sel$call</code></pre>
<pre><code>lm(formula = charges ~ age + bmi + children + smoker + region, 
    data = insurance)</code></pre>
<p>The model given by stepwise selection is same as the model we got by selecting the predictors with significant p-values (works in this case). You can check the summary of the new model to see if there is any improvement in the model.</p>
<hr />
</div>
<div id="are-there-any-multicollinear-features" class="section level2">
<h2>Are there any multicollinear features?</h2>
<p>Multicollinearity in multiple regression is a phenomenon in which two or more predictors are highly related to each other, and hence one predictor can be used to predict the value of the other. The problem with multi-collinearity is that it can make it harder to estimate the individual effects of the predictors on the outcome.</p>
<p>Usually, collinearity between pairs of predictors can be detected using a correlation matrix. However, corrrelation matrix fails, for example, when more than two predictors have high collinearity as a group but don’t have high correlations as pairs.</p>
<p>A better way to detect multicollinearity is Variance Inflation Factor (VIF). VIF of any predictor is the ratio of variance of its estimated coefficient in the full model to the variance of its estimated coefficient when fit on the outcome only by itself (as in simple linear regression). A VIF of 1 indicates no presence of multicollinearity. Usually, a VIF value of above 5 or 10 is taken as an indicator of multicollinearity. The simplest way of getting rid of multicollinearity in that case is to discard the predictor with high value of VIF.</p>
<pre class="r"><code>vif(step.lm.fit) %&gt;% 
  knitr::kable()</code></pre>
<table>
<thead>
<tr class="header">
<th></th>
<th align="right">GVIF</th>
<th align="right">Df</th>
<th align="right">GVIF^(1/(2*Df))</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>age</td>
<td align="right">1.016188</td>
<td align="right">1</td>
<td align="right">1.008061</td>
</tr>
<tr class="even">
<td>bmi</td>
<td align="right">1.104197</td>
<td align="right">1</td>
<td align="right">1.050808</td>
</tr>
<tr class="odd">
<td>children</td>
<td align="right">1.003714</td>
<td align="right">1</td>
<td align="right">1.001855</td>
</tr>
<tr class="even">
<td>smoker</td>
<td align="right">1.006369</td>
<td align="right">1</td>
<td align="right">1.003179</td>
</tr>
<tr class="odd">
<td>region</td>
<td align="right">1.098869</td>
<td align="right">3</td>
<td align="right">1.015838</td>
</tr>
</tbody>
</table>
<p>None of the predictors in our case has a high value of VIF. Hence, we don’t need to worry about multicollinearity in our case.</p>
<div class="figure">
<img src="static/post/2018-09-05-linear-regression-modeling-and-assumptions_files/multicollinearity%20meme.jpg" alt="Source: Google Images" />
<p class="caption">Source: Google Images</p>
</div>
<hr />
</div>
<div id="is-the-relationship-linear" class="section level2">
<h2>Is the relationship linear?</h2>
<p>By applying linear regression, we are assuming that there is a linear relationship between the predictors and the outcome. If the underlying relationship is quite far from linear, then most of the inferences we would make would be doubtful. This also means reduced accuracy of model.</p>
<p>The non-linearity of the model can be determined using the residual plot. <a href="http://www.statisticshowto.com/residual/" target="_blank">Residual</a> for any observation is the difference between the actual outcome and the fitted outcome as per the model. For multiple linear regression, we can plot the residuals versus the fitted values. Presence of a pattern in the residual plot would imply a problem with the linear assumption of the model.</p>
<pre class="r"><code>#type = &quot;rstandard&quot; draws a plot for standardized residuals
residualPlot(step.lm.fit, type = &quot;rstandard&quot;)</code></pre>
<p><img src="/post/2018-09-05-linear-regression-modeling-and-assumptions_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>The blue line represents a smooth pattern between the fitted values and the standard residuals. The curve in our case denotes slight non-linearity in our data.</p>
<p>The non-linearity can be further explored by looking at <a href="https://www.r-bloggers.com/r-regression-diagnostics-part-1/" target="_blank">Component Residual plots</a> (CR plots). CR plots can be created in R using the function <em>ceresPlots()</em>.</p>
<pre class="r"><code>ceresPlots(step.lm.fit)</code></pre>
<p><img src="/post/2018-09-05-linear-regression-modeling-and-assumptions_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<p>The pink line (residual line) is modelled for the relation between the predictor and the residuals. The blue dashed line (component line) is for the line of best fit. A significant difference between the two lines for a predictor implies that the predictor and the outcome don’t have a linear relationship.</p>
<p>This kind of inconsistency can be seen in the CR plot for <em>bmi</em>. Let’s take a closer look:</p>
<pre class="r"><code>ceresPlot(step.lm.fit, variable = &#39;bmi&#39;)</code></pre>
<p><img src="/post/2018-09-05-linear-regression-modeling-and-assumptions_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<p>The difference between the component line and the residual line becomes more clear now.</p>
<p>One of the methods of fixing this is introducing non-linear transformation of predictors of the model. Let’s try adding a non-linear transformation of <em>bmi</em> to the model.</p>
<pre class="r"><code>#update() can be used to update an existing model with new requirements
step.lm.fit.new &lt;- update(step.lm.fit, .~.+I(bmi^1.25))

ceresPlots(step.lm.fit.new)</code></pre>
<p><img src="/post/2018-09-05-linear-regression-modeling-and-assumptions_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<p>The CR plot of bmi no more has a difference between the residual line and the component line. As per the CR plots, the addition of a non-linear transformation of bmi to the existing model fixed the problem.</p>
<p>We can use ANOVA to check if the new model is significantly better than the previous model. A low p-value (&lt;0.05) for the new model will mean we can conclude that it is better than the previous model:</p>
<pre class="r"><code>anova(step.lm.fit, step.lm.fit.new, test = &quot;F&quot;)</code></pre>
<pre><code>Analysis of Variance Table

Model 1: charges ~ age + bmi + children + smoker + region
Model 2: charges ~ age + bmi + children + smoker + region + I(bmi^1.25)
  Res.Df        RSS Df Sum of Sq      F  Pr(&gt;F)  
1   1330 4.8845e+10                              
2   1329 4.8697e+10  1 148484981 4.0524 0.04431 *
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>Since the model with non-linear transformation of <em>bmi</em> has a sufficiently low p-value (&lt;0.05), we can conclude that it is better than the previous model.</p>
<p>Let’s look at the residual plot of this new model.</p>
<pre class="r"><code>residualPlot(step.lm.fit.new, type = &quot;rstandard&quot;)</code></pre>
<p><img src="/post/2018-09-05-linear-regression-modeling-and-assumptions_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<p>Looking at the residual plot of the new model, there is not much change in the overall pattern of the standard residuals.</p>
<div class="figure">
<img src="static/post/2018-09-05-linear-regression-modeling-and-assumptions_files/multiple-regression-more-like-multiple-depression.jpg" alt="Source: Google Images" />
<p class="caption">Source: Google Images</p>
</div>
<p>Another method of fixing the problem of non-linearity is introducing an <a href="https://en.wikipedia.org/wiki/Interaction_(statistics)#In_regression" target="_blank">interaction</a> between some predictors. A person who smokes and has a high bmi may have higher charges as compared to a person who has lower bmi and is a non-smoker. Let’s update the model to introduce an interaction between <em>bmi</em> and <em>smoker</em>, and see if that makes a difference:</p>
<pre class="r"><code>lm.fit1 &lt;- update(step.lm.fit.new, ~ .+bmi*smoker)

residualPlot(lm.fit1, type = &quot;rstandard&quot;, id=TRUE)</code></pre>
<p><img src="/post/2018-09-05-linear-regression-modeling-and-assumptions_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
<pre class="r"><code>anova(step.lm.fit.new, lm.fit1, test = &quot;F&quot;)</code></pre>
<pre><code>Analysis of Variance Table

Model 1: charges ~ age + bmi + children + smoker + region + I(bmi^1.25)
Model 2: charges ~ age + bmi + children + smoker + region + I(bmi^1.25) + 
    bmi:smoker
  Res.Df        RSS Df  Sum of Sq      F    Pr(&gt;F)    
1   1329 4.8697e+10                                   
2   1328 3.1069e+10  1 1.7627e+10 753.45 &lt; 2.2e-16 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>Not only the relation becomes more linear with less appearance of a pattern in the residual plot, the new model is significantly better than the previous model (without interactions) as can be seen with the p-value (&lt;2.2e-16).</p>
<pre class="r"><code>summary(lm.fit1)</code></pre>
<pre><code>
Call:
lm(formula = charges ~ age + bmi + children + smoker + region + 
    I(bmi^1.25) + bmi:smoker, data = insurance)

Residuals:
     Min       1Q   Median       3Q      Max 
-11932.6  -1968.0  -1303.7   -311.3  29983.2 

Coefficients:
                  Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)     -14184.835   4052.086  -3.501 0.000480 ***
age                262.504      9.509  27.607  &lt; 2e-16 ***
bmi               1974.710    659.578   2.994 0.002805 ** 
children           514.273    109.947   4.677  3.2e-06 ***
smokeryes       -20370.230   1644.191 -12.389  &lt; 2e-16 ***
regionnorthwest   -654.158    380.892  -1.717 0.086132 .  
regionsoutheast  -1172.647    382.170  -3.068 0.002196 ** 
regionsouthwest  -1285.747    381.967  -3.366 0.000784 ***
I(bmi^1.25)       -661.819    223.449  -2.962 0.003112 ** 
bmi:smokeryes     1440.617     52.483  27.449  &lt; 2e-16 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

Residual standard error: 4837 on 1328 degrees of freedom
Multiple R-squared:  0.8415,    Adjusted R-squared:  0.8405 
F-statistic: 783.6 on 9 and 1328 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>Looking at the summary of the model, the R-squared is higher now (0.8405), with new model explaining more than 84% variance of the data, and the RSE has decreased too (4837).</p>
<hr />
</div>
<div id="non-constant-variance-of-error-terms" class="section level2">
<h2>Non-constant variance of error terms</h2>
<p>Constant variance (<a href="https://en.wikipedia.org/wiki/Homoscedasticity" target="_blank">homoscedasticity</a>) of errors is another assumption of a linear regression model. The error terms may, for instance, change with the value of the response variable in case of non-constant variance (heteroscedasticity) of errors. Some of the graphical methods of identifying heteroscedasticity is presence of a funnel shape in the residual plot, or existence of a curve in the residual plot. In the above plot, we don’t see any clear pattern.</p>
<p>A statistical way is an extension of the Breusch-Pagan Test, available in R as <em>ncvTest()</em> in the cars package. It assumes a null hypothesis of constant variance of errors against the alternate hypothesis that the error variance changes with the level of the response or with a linear combination of predictors.</p>
<pre class="r"><code># Evaluate homoscedasticity
# non-constant error variance test
ncvTest(lm.fit1)</code></pre>
<pre><code>Non-constant Variance Score Test 
Variance formula: ~ fitted.values 
Chisquare = 17.90486, Df = 1, p = 2.3223e-05</code></pre>
<p>A very low p-value (~2.3-05) means the null hypothesis can be rejected. In other words, there is a high chance that the errors have a non-constant variance.</p>
<p>One of the methods to fix this problem is transformation of the outcome variable.</p>
<pre class="r"><code>yTransformer &lt;- 0.8

trans.lm.fit &lt;- update(lm.fit1, charges^yTransformer~.)

# Evaluate homoscedasticity
# non-constant error variance test
ncvTest(trans.lm.fit)</code></pre>
<pre><code>Non-constant Variance Score Test 
Variance formula: ~ fitted.values 
Chisquare = 0.005724708, Df = 1, p = 0.93969</code></pre>
<pre class="r"><code>residualPlot(trans.lm.fit, type = &quot;rstandard&quot;, id=T)</code></pre>
<p><img src="/post/2018-09-05-linear-regression-modeling-and-assumptions_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
<p>A p-value of ~0.94 implies here that we cannot reject the null hypothesis of constant variance of error terms.</p>
<div class="figure">
<img src="static/post/2018-09-05-linear-regression-modeling-and-assumptions_files/2d3a442e8b01577630a816aea1ba2ac8.jpeg" alt="Source: Google Images" />
<p class="caption">Source: Google Images</p>
</div>
<p>However, there is a slight increase in non-linearity of the model as can be seen in the residual plot.</p>
<p>This can be fixed further by looking at relations between individual predictors and outcome.</p>
<hr />
</div>
<div id="correlation-of-error-terms" class="section level2">
<h2>Correlation of error terms</h2>
<p>An important assumption of linear regression model is that the consecutive error terms are uncorrelated. The standard errors of the estimated regression coefficients are calculated on the basis of this assumption. If the consecutive error terms are correlated, the standard errors of the estimated regression coefficients may be much larger.</p>
<p>We can check the auto-correlation of error terms using the <a href="https://en.wikipedia.org/wiki/Durbin–Watson_statistic" target="_blank">Durbin-Watson test</a>. The null hypothesis is that the consecutive errors have no auto-correlation. The alternate hypothesis is that the the consecutive errors have a statistically significant correlation:</p>
<pre class="r"><code>set.seed(1)
# Test for Autocorrelated Errors
durbinWatsonTest(trans.lm.fit, max.lag = 5, reps=1000)</code></pre>
<pre><code> lag Autocorrelation D-W Statistic p-value
   1    -0.036139510      2.070557   0.216
   2    -0.026396886      2.050927   0.340
   3    -0.009537725      2.017017   0.712
   4    -0.004187672      1.996569   0.972
   5     0.008894177      1.970058   0.680
 Alternative hypothesis: rho[lag] != 0</code></pre>
<p>Here we are checking for auto-correlation of residuals for 5 different lags. The p-value for none of the lags is less than 0.05. Hence, we cannot reject the null hypothesis that the consecutive errors are not correlated, concluding that the consecutive errors are not correlated.</p>
<hr />
</div>
<div id="outliers" class="section level2">
<h2>Outliers</h2>
<p>Outliers are the observations which in some way are quite different from the distribution of the data. With respect to a model, an outlier is an observation whose predicted outcome is much different from the actual value of the outcome.</p>
<p>Residual Plots (yes, again) can be used to identify outliers. We can look at the the last residual plot above. To make a standard comparison of residuals, we can use standardized residuals as in that plot. Usually, the observations with absolute standard residuals above 3 are possible outliers.</p>
<pre class="r"><code>#finding ids of observations with absolute standard residuals of 3+, and order by value in desc order
pot.outliers &lt;- stdres(trans.lm.fit) %&gt;%
  tidy() %&gt;%
  dplyr::filter(abs(x)&gt;3) %&gt;%
  dplyr::arrange(-x)
pot.outliers</code></pre>
<pre><code># A tibble: 51 x 2
   names     x
   &lt;chr&gt; &lt;dbl&gt;
 1 517    5.31
 2 1301   5.11
 3 220    4.85
 4 1020   4.67
 5 431    4.62
 6 243    4.59
 7 1207   4.48
 8 527    4.42
 9 1028   4.33
10 937    4.25
# ... with 41 more rows</code></pre>
<pre class="r"><code>outlier.ids &lt;- as.numeric(pot.outliers$names)</code></pre>
<p>51 observations have an absolute standardized residual greater than 3. That marks approximately 3.8% of the observations. This is a high percentage and further analysis should be done to identify the reason behind outliers (which is a totally different topic). For now, we will remove these outliers and build our model.</p>
<pre class="r"><code>clean.insurance &lt;- insurance %&gt;%
  dplyr::slice(-(outlier.ids))

#fitting the model on data after removing the outliers
lm.fit2 &lt;- update(trans.lm.fit, .~., data = clean.insurance) 
summary(lm.fit2)</code></pre>
<pre><code>
Call:
lm(formula = charges^yTransformer ~ age + bmi + children + smoker + 
    region + I(bmi^1.25) + bmi:smoker, data = clean.insurance)

Residuals:
     Min       1Q   Median       3Q      Max 
-1081.34  -137.51   -73.18    26.35  1739.12 

Coefficients:
                  Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)     -1405.6648   315.5522  -4.455 9.14e-06 ***
age                34.4895     0.7455  46.262  &lt; 2e-16 ***
bmi               201.4243    51.3600   3.922 9.26e-05 ***
children           65.8613     8.5713   7.684 3.06e-14 ***
smokeryes       -1726.8579   127.7735 -13.515  &lt; 2e-16 ***
regionnorthwest  -103.9364    30.0050  -3.464  0.00055 ***
regionsoutheast  -130.2721    29.9565  -4.349 1.48e-05 ***
regionsouthwest  -124.5118    29.8996  -4.164 3.33e-05 ***
I(bmi^1.25)       -67.1363    17.3982  -3.859  0.00012 ***
bmi:smokeryes     145.8096     4.0753  35.779  &lt; 2e-16 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

Residual standard error: 371.5 on 1277 degrees of freedom
Multiple R-squared:  0.9245,    Adjusted R-squared:  0.9239 
F-statistic:  1737 on 9 and 1277 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>We can’t compare the R-squared now since the model is built by removing the outliers.</p>
<hr />
</div>
<div id="interpretations" class="section level2">
<h2>Interpretations</h2>
<p>Let’s look at the actual charges vs fitted values for the model. Before doing that, let’s look at the how the fitted values of the very first model that we created stand against the actual outcomes:</p>
<pre class="r"><code>predictions &lt;- predict(lm.fit, insurance, interval = &quot;confidence&quot;) %&gt;%
  tidy()

ggplot(predictions, aes(x=insurance$charges, y=fit))+
  geom_point()+
  geom_smooth(aes(color = &#39;model&#39;))+
  geom_line(aes(x=seq(min(insurance$charges),max(insurance$charges), length.out = 1338), 
                y=seq(min(insurance$charges),max(insurance$charges), length.out = 1338), 
                color = &#39;ideal&#39;))+
  labs(x=&quot;actual charges&quot;, y=&quot;fitted values&quot;) + 
  scale_color_manual(&#39;linear relation&#39;, values = c(&#39;red&#39;, &#39;blue&#39;)) +
  theme(legend.position = c(0.8, 0.2)) </code></pre>
<p><img src="/post/2018-09-05-linear-regression-modeling-and-assumptions_files/figure-html/unnamed-chunk-21-1.png" width="672" /></p>
<p>The initial model is able to approximate the actual charges below $17,000, but as the actual charges go above $20,000, the gap between actual charges and fitted values keeps increasing. As per the initial model, the actual charges near $50,000 are fitted as somewhere near or below $40,000, and this gap keeps increasing upwards.</p>
<p>In comparison, this is how the fitted values of the last model look against the actual outcomes:</p>
<pre class="r"><code>predictions &lt;- predict(lm.fit2, insurance, interval = &quot;confidence&quot;)^(1/yTransformer) %&gt;%
  tidy()

ggplot(predictions, aes(x=insurance$charges, y=fit))+
  geom_point()+
  geom_smooth(aes(color = &#39;model&#39;))+
  geom_line(aes(x=seq(min(insurance$charges),max(insurance$charges), 
                      length.out = 1338), 
                y=seq(min(insurance$charges),max(insurance$charges), 
                      length.out = 1338), 
                color = &#39;ideal&#39;))+
  labs(x=&quot;actual charges&quot;, y=&quot;fitted values&quot;) + 
  scale_color_manual(&#39;relation&#39;, values = c(&#39;red&#39;, &#39;blue&#39;)) +
  theme(legend.position = c(0.8, 0.2)) </code></pre>
<p><img src="/post/2018-09-05-linear-regression-modeling-and-assumptions_files/figure-html/unnamed-chunk-22-1.png" width="672" /></p>
<p>This is quite an improvement from the initial model. The fitted values are closer to the actual charges, as can be seen by the gaps between the ideal relation and the modelled relation between the actual charges and fitted values.</p>
<p>There is still a lot of scope of improvement. This model still has a lot of non-constant variance of errors, as can be seen from the varying deviations of fitted values along the line of the ideal relation.</p>
<p>We can look at the estimated coefficients of the predictors and their confidence intervals for interpretation on how they define the model.</p>
<pre class="r"><code>confint(lm.fit2) %&gt;%
  tidy() %&gt;%
  add_column(coefficients = lm.fit2$coefficients, .after = 2) %&gt;%
  knitr::kable()</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">.rownames</th>
<th align="right">X2.5..</th>
<th align="right">coefficients</th>
<th align="right">X97.5..</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">(Intercept)</td>
<td align="right">-2024.72245</td>
<td align="right">-1405.66485</td>
<td align="right">-786.60725</td>
</tr>
<tr class="even">
<td align="left">age</td>
<td align="right">33.02690</td>
<td align="right">34.48948</td>
<td align="right">35.95205</td>
</tr>
<tr class="odd">
<td align="left">bmi</td>
<td align="right">100.66518</td>
<td align="right">201.42433</td>
<td align="right">302.18349</td>
</tr>
<tr class="even">
<td align="left">children</td>
<td align="right">49.04596</td>
<td align="right">65.86134</td>
<td align="right">82.67673</td>
</tr>
<tr class="odd">
<td align="left">smokeryes</td>
<td align="right">-1977.52699</td>
<td align="right">-1726.85792</td>
<td align="right">-1476.18886</td>
</tr>
<tr class="even">
<td align="left">regionnorthwest</td>
<td align="right">-162.80104</td>
<td align="right">-103.93644</td>
<td align="right">-45.07185</td>
</tr>
<tr class="odd">
<td align="left">regionsoutheast</td>
<td align="right">-189.04134</td>
<td align="right">-130.27207</td>
<td align="right">-71.50279</td>
</tr>
<tr class="even">
<td align="left">regionsouthwest</td>
<td align="right">-183.16949</td>
<td align="right">-124.51184</td>
<td align="right">-65.85419</td>
</tr>
<tr class="odd">
<td align="left">I(bmi^1.25)</td>
<td align="right">-101.26845</td>
<td align="right">-67.13632</td>
<td align="right">-33.00419</td>
</tr>
<tr class="even">
<td align="left">bmi:smokeryes</td>
<td align="right">137.81459</td>
<td align="right">145.80964</td>
<td align="right">153.80468</td>
</tr>
</tbody>
</table>
<p>In the above table, the X2.5 and X97.5 mark the lower and upper bounds for the 95% confidence interval of the regression coefficients. These are calculated using the standard errors of the estimated coefficients. As an example, for <em>age</em>, the estimated coefficient is ~34.49 and the 95% confidence interval lies between ~33.03 and ~35.95. This means that as per the model, keeping everything else fixed, an increase in 1 year of age will result in an increase of 34.49 in the value of <span class="math display">\[charges^{0.8}\]</span> (since we transformed the outcome). However, this is an estimate and hence there is a scope for variation. This variation is accounted for by the confidence interval, denoting that about 95% of the times, the change in the value of charges^0.8 will be between 33.03 and 35.95, keeping everything else fixed.</p>
<p>This effect can be further viewed by plotting the effect using <em>effects</em> function.</p>
<pre class="r"><code>#get effects for age
effs &lt;- effect(&#39;age&#39;, mod = lm.fit2, 
       xlevels = list(age=min(insurance$age):max(insurance$age)))

#create a data frame for age and the fitted values, with upper and lower confidence bands
model.effs &lt;- effs[c(&#39;x&#39;, &#39;lower&#39;, &#39;fit&#39;, &#39;upper&#39;)] %&gt;%
  as.data.frame()

model.effs$fit &lt;- model.effs$fit^(1/yTransformer)
model.effs$lower &lt;- model.effs$lower^(1/yTransformer)
model.effs$upper &lt;- model.effs$upper^(1/yTransformer)

ggplot(model.effs, aes(x = age, y = fit)) +
  theme_bw()+
  geom_smooth()+
  geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.5)</code></pre>
<p><img src="/post/2018-09-05-linear-regression-modeling-and-assumptions_files/figure-html/unnamed-chunk-24-1.png" width="672" /></p>
<p>For an average value of other predictors, insurance charges increase with increase in age.</p>
<p>More interesting effects can be seen for the interaction between <em>bmi</em> and <em>smoker</em>.</p>
<pre class="r"><code>#get effects for bmi*smoker
effs &lt;- effect(&#39;bmi*smoker&#39;, mod = lm.fit2, 
       xlevels = list(bmi=min(insurance$bmi):max(insurance$bmi)))

#create a data frame for bmi*smoker and the fitted values, with upper and lower confidence bands
model.effs &lt;- effs[c(&#39;x&#39;, &#39;lower&#39;, &#39;fit&#39;, &#39;upper&#39;)] %&gt;%
  as.data.frame()

model.effs$fit &lt;- model.effs$fit^(1/yTransformer)
model.effs$lower &lt;- model.effs$lower^(1/yTransformer)
model.effs$upper &lt;- model.effs$upper^(1/yTransformer)

ggplot(model.effs, aes(x = x.bmi, y = fit)) +
  facet_wrap(~x.smoker)+
  theme_bw()+
  geom_smooth(size=0.5)+
  geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.5)</code></pre>
<p><img src="/post/2018-09-05-linear-regression-modeling-and-assumptions_files/figure-html/unnamed-chunk-25-1.png" width="672" /></p>
<p>Non-smokers, irrespective of their bmi, have mostly low insurance charges for an average value of other predictors. Smokers with low bmi have low insurance charges, though still higher than non-smokers with any value of bmi. Moreover, as their bmi increases, the insurance charges of smokers increases rapidly.</p>
<hr />
</div>
<div id="conclusion" class="section level2">
<h2>Conclusion</h2>
<p>The model we have built can be used for inference of how the different predictors influence the outcome.</p>
<p>It is far from perfect. Their is still presence of non-linearity and non-constant variance of errors. Moreover, the outliers and leverage points should be analyzed further to find a better model. It may not (and most probably won’t) give similar results when used to predict the outcome for new, unseen data. In order to use it for prediction, more concrete measures should be taken for ensuring the accuracy of the model, like cross-validation.</p>
<p>It still helps by providing good estimations of the significant relations between the predictors and the outcome, which can themselves be used to summarize the data in a more useful and presentful way.</p>
<hr />
<div id="sources" class="section level3">
<h3>Sources :</h3>
<ul>
<li><a href="https://www.kaggle.com/mirichoi0218/insurance/home" class="uri" target="_blank">https://www.kaggle.com/mirichoi0218/insurance/home</a></li>
<li>An Introduction to Statistical Learning and Reasoning</li>
<li>Wikipedia</li>
<li><a href="https://www.statmethods.net/stats/rdiagnostics.html" class="uri" target="_blank">https://www.statmethods.net/stats/rdiagnostics.html</a></li>
<li><a href="https://www.statmethods.net/stats/regression.html" class="uri" target="_blank">https://www.statmethods.net/stats/regression.html</a></li>
<li><a href="https://datascienceplus.com/how-to-detect-heteroscedasticity-and-rectify-it/" class="uri" target="_blank">https://datascienceplus.com/how-to-detect-heteroscedasticity-and-rectify-it/</a></li>
<li><a href="http://www.statisticshowto.com/probability-and-statistics/f-statistic-value-test/" class="uri" target="_blank">http://www.statisticshowto.com/probability-and-statistics/f-statistic-value-test/</a></li>
</ul>
</div>
</div>

    </div>

    


<div class="article-tags">
  
  <a class="btn btn-primary btn-outline" href="/tags/data-analysis">data analysis</a>
  
  <a class="btn btn-primary btn-outline" href="/tags/rstats">rstats</a>
  
  <a class="btn btn-primary btn-outline" href="/tags/statistics">statistics</a>
  
  <a class="btn btn-primary btn-outline" href="/tags/statistical-learning">statistical learning</a>
  
</div>




    
    
    <div class="article-widget">
      <div class="hr-light"></div>
      <h3>Related</h3>
      <ul>
        
        <li><a href="/post/exploratory-data-analysis/">Exploratory Data Analysis</a></li>
        
      </ul>
    </div>
    

    

    
<section id="comments">
  <div id="disqus_thread"></div>
<script>
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "krohitm" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</section>



  </div>
</article>

<footer class="site-footer">
  <div class="container">
    <p class="powered-by">

      

      Powered by the
      <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
      <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

      <span class="pull-right" aria-hidden="true">
        <a href="#" id="back_to_top">
          <span class="button_icon">
            <i class="fa fa-chevron-up fa-2x"></i>
          </span>
        </a>
      </span>

    </p>
  </div>
</footer>


<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <button type="button" class="close btn-large" data-dismiss="modal">&times;</button>
        <h4 class="modal-title">Cite</h4>
      </div>
      <div>
        <pre><code class="modal-body tex"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-primary btn-outline js-copy-cite" href="#" target="_blank">
          <i class="fa fa-copy"></i> Copy
        </a>
        <a class="btn btn-primary btn-outline js-download-cite" href="#" target="_blank">
          <i class="fa fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

    

    
    
    <script id="dsq-count-scr" src="//krohitm.disqus.com/count.js" async></script>
    

    

    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.2.1/jquery.min.js" integrity="sha512-3P8rXCuGJdNZOnUx/03c1jOTnMn3rP63nBip5gOP2qmUh5YAdVAvFZ1E+QLZZbC1rtMrQb+mah3AfYW11RUrWA==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.3/imagesloaded.pkgd.min.js" integrity="sha512-umsR78NN0D23AzgoZ11K7raBD+R6hqKojyBZs1w8WvYlsI+QuKRGBx3LFCwhatzBunCjDuJpDHwxD13sLMbpRA==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha512-iztkobsvnjKfAtTNdHkGVjAYTrrtlC7mGp/54c40wowO7LhURYl3gVzzcEqGl/qKXQltJ2HwMrdLcNUdo+N/RQ==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.4/isotope.pkgd.min.js" integrity="sha512-VDBOIlDbuC4VWxGJNmuFRQ0Li0SKkDpmGyuhAG5LTDLd/dJ/S0WMVxriR2Y+CyPL5gzjpN4f/6iqWVBJlht0tQ==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>
    
    
    <script src="/js/hugo-academic.js"></script>
    

    
    
      
      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js" integrity="sha256-/BfiIkHlHoVihZdc6TFuj7MmJ0TWcWsMXkeDFwhi0zw=" crossorigin="anonymous"></script>
      

      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/R.min.js"></script>
      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/python.min.js"></script>
      

      

      <script>hljs.initHighlightingOnLoad();</script>
    

    
    

  </body>
</html>

