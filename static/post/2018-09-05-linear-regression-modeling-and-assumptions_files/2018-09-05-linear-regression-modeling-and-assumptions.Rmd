---
title: 'Linear regression: Modeling and Assumptions'
author: Kumar Rohit Malhotra
date: '2018-09-09'
slug: linear-regression-modeling-and-assumptions
categories:
  - data analysis
tags:
  - data analysis
  - rstats
  - statistics
  - statistical learning
---

```{r setup, include=FALSE, message=FALSE, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message=FALSE, comment='')
```

Regression analysis is a powerful statistical process to find the relations within a dataset, with the key focus being on relationships between the independent variables (predictors) and a dependent variable (outcome). It can be used to build models for inference or prediction. Among several methods of regression analysis, linear regression sets the basis and is quite widely used for [several real-world applications](https://en.wikipedia.org/wiki/Linear_regression#Applications){target="_blank"}. 

![Source : Google Images](/post/2018-09-05-linear-regression-modeling-and-assumptions_files/2120406.jpg)

In this post, we will look at building a linear regression model for inference. The dataset we will use is the insurance charges data obtained from [Kaggle](https://www.kaggle.com/mirichoi0218/insurance/home){target="_blank"}. This data set consists of 1,338 observations and 7 columns: age, sex, bmi, children, smoker, region and charges.

The key questions that we would be asking are:

1. Is there a relationship between medical charges and other variables in the dataset?
2. How valid is the model we have built?
3. What can we do to improve the model?

We start with importing the required libraries and data:

```{r}
library(magrittr)
library(purrr)
library(MASS)
library(car)
library(broom)
library(ggplot2)
library(psych)
library(caret)
library(tidyr)
library(effects)
library(tibble)
```

```{r}
insurance <- read.csv('~/Documents/CodeWork/medicalCost/insurance.csv')
summary(insurance)
```

Some simple observations that can be taken from the summary are:

1. The age of participants varies from 18 to 64.
2. Around 49.48% of participants are female.
3. The bmi of participants ranges from 15.96 to 53.13.
4. Only 20.48% of the participants are smokers.

Let's start with building a linear model. Instead of simple linear regression, where you have one predictor and one outcome, we will go with multiple linear regression, where you have more than one predictors and one outcome.

Multiple linear regression follows the formula : 

$y = {\beta_0}+ {\beta_1}x_1+{\beta_2}x_2+...$

The coefficients in this linear equation denote the magnitude of additive relation between the predictor and the response. In simpler words, keeping everything else fixed, a unit change in $x_1$ will lead to change of $\beta_1$ in the outcome, and so on.

## Is there a relationship between the medical charges and the predictors?

Our first step is finding if there is any relationship between the outcome and the predictors.

The null hypothesis would be that there is no relation between any of the predictors and the response, which would be possible when all the regression coefficients for the predictors are equal to zero. The alternate hypothesis would be that atleast one of the predictors has a relation with the outcome, that is the regression coefficient of one of the predictors is non-zero.

This hypothesis is tested by computing the [F statistic](http://www.statisticshowto.com/probability-and-statistics/F statistic-value-test/){target="_blank"}. In case of no relationship between the predictor and the response, F statistic will be closer to 1. On the contrary, if the alternate hypothesis is true, the F statistic will be greater than 1. The p-value of F statistic can be used to determine whether the null hypothesis can be rejected or not.

We will start with fitting a multiple linear regression model using all the predictors: 

```{r}
lm.fit <- lm(formula = charges~., data = insurance)
#Here '.' means we are using all the predictors in the dataset.
summary(lm.fit)
```

A high value of F statistic, with a very low p-value (<2.2e-16), implies that the null hypothesis can be rejected. This means there is a potential relationship between the predictors and the outcome.

RSE (Residual Standard Error) is the estimate of standard deviation of irreducible error (the error which can't be reduced even if knew the true regression line; hence, irreducible). In simpler words, it is the average deviation between the actual outcome and the true regression line. Hence, a large value of RSE means a high deviation from the true regression line. As such, RSE is useful in determining the lack of fit of the model to the data. RSE in our model is large (6062), indicating that the model doesn't fit the data well.

R-squared measures the proportion of variability in the outcome that can be explained by the predictor, and is always between 0 and 1; the higher the value, the better the model is able to explain the variability in the outcome. However, increase in number of predictors mostly results in an increased value of R-squared due to [inflation of R-squared](https://en.wikipedia.org/wiki/Coefficient_of_determination#Inflation_of_R2){target="_blank"}. [Adjusted R-squared](https://en.wikipedia.org/wiki/Coefficient_of_determination#Adjusted_R2){target="_blank"} adjusts the value of R-squared to avoid this effect. A high value of adjusted R-squared (0.7494) shows that more than 74% of the variance in the data is being explained by the model. From now on, we will use the term R-squared for adjusted R-squared.

The Std. Error gives us the average amount that the estimated coefficient of a predictor differs from the actual coefficient of predictor. It can be used to compute the confidence interval of an estimated coefficient, which we will see later.

The _t value_ of a predictor tells us how many standard deviations its estimated coefficient is away from 0. _Pr (>|t|)_ for a predictor is the p-value for the estimated regression coefficient, which is same as saying what is the probability of seeing a t value for the regression coefficient. A very low p-value (<0.05) for a predictor can be used to infer that there is a relationsip between the predictor and the outcome.

Our next step should be [validation of regression analyis](https://en.wikipedia.org/wiki/Regression_validation){target="_blank"}. This may mean validation of underlying assumptions of the model, checking the structure of model with different predictors, looking for values that have an exceptionally large impact on the regression model, looking for observations that have not been represented well enough in the model, and more. 

In the rest of the post, we will look at some of these methods of model validation and improvement.

***

## Which variables have a strong relation to medical charges?

Now that we have determined that there is a relation between the predictors and the outcome, our next step would be finding out if all or only some of the predictors are related to the outcome.

If we look at the p-values of the estimated coefficients above, we see that not all the coefficients are statistically significant (<0.05). This means that only a subset of the predictors are related to the outcome.

We can look at the individual p-values for selecting the variables. This may not be a problem when the number of predictors (7) is quite small compared to the number of observations (1338). This method won't, however, work when the number of predictors is greater than the number of observations because of the [multiple testing problem](http://www.statisticshowto.com/multiple-testing-problem/){target="_blank"}. In such cases, we would have to use the [feature/variable selection](https://en.wikipedia.org/wiki/Feature_selection){target="_blank"} methods, like forward selection, backward selection, or mixed selection. 

![Source: Google Images](/post/2018-09-05-linear-regression-modeling-and-assumptions_files/RegressionMeme.jpg)

Before jumping on to feature selection using any of these methods, let us try linear regression using the features with significant p-values only.

```{r}
lm.fit.sel <- lm(charges~age+bmi+children+smoker+region, data = insurance)
```

We will compare this to [mixed selection](https://www.stat.ubc.ca/~rollin/teach/643w04/lec/node43.html){target="_blank"}, which is a combination of forward and backward selection. This can be done in R using the _stepAIC()_ function, which uses [Akaike Information Criterion](https://en.wikipedia.org/wiki/Akaike_information_criterion){target="_blank"} (AIC) to select the best model out of multiple models.

```{r}
#selecting direction = "both" for mixed selection
step.lm.fit <- stepAIC(lm.fit, direction = "both", trace = FALSE)
```

Let's compare the two models :

```{r}
step.lm.fit$call
lm.fit.sel$call
```

The model given by stepwise selection is same as the model we got by selecting the predictors with significant p-values (works in this case). You can check the summary of the new model to see if there is any improvement in the model.

***

## Are there any multicollinear features?

Multicollinearity in multiple regression is a phenomenon in which two or more predictors are highly related to each other, and hence one predictor can be used to predict the value of the other. The problem with multi-collinearity is that it can make it harder to estimate the individual effects of the predictors on the outcome. 

Usually, collinearity between pairs of predictors can be detected using a correlation matrix. However, corrrelation matrix fails, for example, when more than two predictors have high collinearity as a group but don't have high correlations as pairs.

A better way to detect multicollinearity is Variance Inflation Factor (VIF). VIF of any predictor is the ratio of variance of its estimated coefficient in the full model to the variance of its estimated coefficient when fit on the outcome only by itself (as in simple linear regression). A VIF of 1 indicates no presence of multicollinearity. Usually, a VIF value of above 5 or 10 is taken as an indicator of multicollinearity. The simplest way of getting rid of multicollinearity in that case is to discard the predictor with high value of VIF.

```{r results='asis'}
vif(step.lm.fit) %>% 
  knitr::kable()
```

None of the predictors in our case has a high value of VIF. Hence, we don't need to worry about multicollinearity in our case.

![Source: Google Images](/post/2018-09-05-linear-regression-modeling-and-assumptions_files/multicollinearity meme.jpg)

***

## Is the relationship linear?

By applying linear regression, we are assuming that there is a linear relationship between the predictors and the outcome. If the underlying relationship is quite far from linear, then most of the inferences we would make would be doubtful. This also means reduced accuracy of model.

The non-linearity of the model can be determined using the residual plot. [Residual](http://www.statisticshowto.com/residual/){target="_blank"} for any observation is the difference between the actual outcome and the fitted outcome as per the model. For multiple linear regression, we can plot the residuals versus the fitted values. Presence of a pattern in the residual plot would imply a problem with the linear assumption of the model.

```{r}
#type = "rstandard" draws a plot for standardized residuals
residualPlot(step.lm.fit, type = "rstandard")
```

The blue line represents a smooth pattern between the fitted values and the standard residuals. The curve in our case denotes slight non-linearity in our data.

The non-linearity can be further explored by looking at [Component Residual plots](https://www.r-bloggers.com/r-regression-diagnostics-part-1/){target="_blank"} (CR plots). CR plots can be created in R using the function _ceresPlots()_.

```{r}
ceresPlots(step.lm.fit)
```

The pink line (residual line) is modelled for the relation between the predictor and the residuals. The blue dashed line (component line) is for the line of best fit. A significant difference between the two lines for a predictor implies that the predictor and the outcome don't have a linear relationship.

This kind of inconsistency can be seen in the CR plot for _bmi_. Let's take a closer look:

```{r}
ceresPlot(step.lm.fit, variable = 'bmi')
```

The difference between the component line and the residual line becomes more clear now. 

One of the methods of fixing this is introducing non-linear transformation of predictors of the model. Let's try adding a non-linear transformation of _bmi_ to the model.

```{r}
#update() can be used to update an existing model with new requirements
step.lm.fit.new <- update(step.lm.fit, .~.+I(bmi^1.25))

ceresPlots(step.lm.fit.new)
```

The CR plot of bmi no more has a difference between the residual line and the component line. As per the CR plots, the addition of a non-linear transformation of bmi to the existing model fixed the problem.

We can use ANOVA to check if the new model is significantly better than the previous model. A low p-value (<0.05) for the new model will mean we can conclude that it is better than the previous model:

```{r}
anova(step.lm.fit, step.lm.fit.new, test = "F")
```

Since the model with non-linear transformation of _bmi_ has a sufficiently low p-value (<0.05), we can conclude that it is better than the previous model.

Let's look at the residual plot of this new model.

```{r}
residualPlot(step.lm.fit.new, type = "rstandard")
```

Looking at the residual plot of the new model, there is not much change in the overall pattern of the standard residuals.

![Source: Google Images](/post/2018-09-05-linear-regression-modeling-and-assumptions_files/multiple-regression-more-like-multiple-depression.jpg)

Another method of fixing the problem of non-linearity is introducing an [interaction](https://en.wikipedia.org/wiki/Interaction_(statistics)#In_regression){target="_blank"} between some predictors. A person who smokes and has a high bmi may have higher charges as compared to a person who has lower bmi and is a non-smoker. Let's update the model to introduce an interaction between _bmi_ and _smoker_, and see if that makes a difference:

```{r}
lm.fit1 <- update(step.lm.fit.new, ~ .+bmi*smoker)

residualPlot(lm.fit1, type = "rstandard", id=TRUE)
anova(step.lm.fit.new, lm.fit1, test = "F")
```

Not only the relation becomes more linear with less appearance of a pattern in the residual plot, the new model is significantly better than the previous model (without interactions) as can be seen with the p-value (<2.2e-16).

```{r}
summary(lm.fit1)
```

Looking at the summary of the model, the R-squared is higher now (0.8405), with new model explaining more than 84% variance of the data, and the RSE has decreased too (4837).

***

## Non-constant variance of error terms

Constant variance ([homoscedasticity](https://en.wikipedia.org/wiki/Homoscedasticity){target="_blank"}) of errors is another assumption of a linear regression model. The error terms may, for instance, change with the value of the response variable in case of non-constant variance (heteroscedasticity) of errors. Some of the graphical methods of identifying heteroscedasticity is presence of a funnel shape in the residual plot, or existence of a curve in the residual plot. In the above plot, we don't see any clear pattern. 

A statistical way is an extension of the Breusch-Pagan Test, available in R as _ncvTest()_ in the cars package. It assumes a null hypothesis of constant variance of errors against the alternate hypothesis that the error variance changes with the level of the response or with a linear combination of predictors.

```{r}
# Evaluate homoscedasticity
# non-constant error variance test
ncvTest(lm.fit1)
```

A very low p-value (~2.3-05) means the null hypothesis can be rejected. In other words, there is a high chance that the errors have a non-constant variance.

One of the methods to fix this problem is transformation of the outcome variable. 

```{r}
yTransformer <- 0.8

trans.lm.fit <- update(lm.fit1, charges^yTransformer~.)

# Evaluate homoscedasticity
# non-constant error variance test
ncvTest(trans.lm.fit)

residualPlot(trans.lm.fit, type = "rstandard", id=T)
```

A p-value of ~0.94 implies here that we cannot reject the null hypothesis of constant variance of error terms. 

![Source: Google Images](/post/2018-09-05-linear-regression-modeling-and-assumptions_files/2d3a442e8b01577630a816aea1ba2ac8.jpeg)

However, there is a slight increase in non-linearity of the model as can be seen in the residual plot.

This can be fixed further by looking at relations between individual predictors and outcome.

***

## Correlation of error terms

An important assumption of linear regression model is that the consecutive error terms are uncorrelated. The standard errors of the estimated regression coefficients are calculated on the basis of this assumption. If the consecutive error terms are correlated, the standard errors of the estimated regression coefficients may be much larger.

We can check the auto-correlation of error terms using the [Durbin-Watson test](https://en.wikipedia.org/wiki/Durbinâ€“Watson_statistic){target="_blank"}. The null hypothesis is that the consecutive errors have no auto-correlation. The alternate hypothesis is that the the consecutive errors have a statistically significant correlation:

```{r}
set.seed(1)
# Test for Autocorrelated Errors
durbinWatsonTest(trans.lm.fit, max.lag = 5, reps=1000)
```

Here we are checking for auto-correlation of residuals for 5 different lags. The p-value for none of the lags is less than 0.05. Hence, we cannot reject the null hypothesis that the consecutive errors are not correlated, concluding that the consecutive errors are not correlated.

***

## Outliers

Outliers are the observations which in some way are quite different from the distribution of the data. With respect to a model, an outlier is an observation whose predicted outcome is much different from the actual value of the outcome.

Residual Plots (yes, again) can be used to identify outliers. We can look at the the last residual plot above. To make a standard comparison of residuals, we can use standardized residuals as in that plot. Usually, the observations with absolute standard residuals above 3 are possible outliers.

```{r}
#finding ids of observations with absolute standard residuals of 3+, and order by value in desc order
pot.outliers <- stdres(trans.lm.fit) %>%
  tidy() %>%
  dplyr::filter(abs(x)>3) %>%
  dplyr::arrange(-x)
pot.outliers
outlier.ids <- as.numeric(pot.outliers$names)
```

51 observations have an absolute standardized residual greater than 3. That marks approximately 3.8% of the observations. This is a high percentage and further analysis should be done to identify the reason behind outliers (which is a totally different topic). For now, we will remove these outliers and build our model.

```{r}
clean.insurance <- insurance %>%
  dplyr::slice(-(outlier.ids))

#fitting the model on data after removing the outliers
lm.fit2 <- update(trans.lm.fit, .~., data = clean.insurance) 
summary(lm.fit2)
```

We can't compare the R-squared now since the model is built by removing the outliers.

***

## Interpretations

Let's look at the actual charges vs fitted values for the model. Before doing that, let's look at the how the fitted values of the very first model that we created stand against the actual outcomes:

```{r}
predictions <- predict(lm.fit, insurance, interval = "confidence") %>%
  tidy()

ggplot(predictions, aes(x=insurance$charges, y=fit))+
  geom_point()+
  geom_smooth(aes(color = 'model'))+
  geom_line(aes(x=seq(min(insurance$charges),max(insurance$charges), length.out = 1338), 
                y=seq(min(insurance$charges),max(insurance$charges), length.out = 1338), 
                color = 'ideal'))+
  labs(x="actual charges", y="fitted values") + 
  scale_color_manual('linear relation', values = c('red', 'blue')) +
  theme(legend.position = c(0.8, 0.2)) 
```

The initial model is able to approximate the actual charges below 17,000 USD, but as the actual charges go above 20,000 USD, the gap between actual charges and fitted values keeps increasing. As per the initial model, the actual charges near 50,000 USD are fitted as somewhere near or below 40,000 USD, and this gap keeps increasing upwards.

In comparison, this is how the fitted values of the last model look against the actual outcomes:

```{r}
predictions <- predict(lm.fit2, insurance, interval = "confidence")^(1/yTransformer) %>%
  tidy()

ggplot(predictions, aes(x=insurance$charges, y=fit))+
  geom_point()+
  geom_smooth(aes(color = 'model'))+
  geom_line(aes(x=seq(min(insurance$charges),max(insurance$charges), 
                      length.out = 1338), 
                y=seq(min(insurance$charges),max(insurance$charges), 
                      length.out = 1338), 
                color = 'ideal'))+
  labs(x="actual charges", y="fitted values") + 
  scale_color_manual('relation', values = c('red', 'blue')) +
  theme(legend.position = c(0.8, 0.2)) 
```

This is quite an improvement from the initial model. The fitted values are closer to the actual charges, as can be seen by the gaps between the ideal relation and the modelled relation between the actual charges and fitted values.

There is still a lot of scope of improvement. This model still has a lot of non-constant variance of errors, as can be seen from the varying deviations of fitted values along the line of the ideal relation.

We can look at the estimated coefficients of the predictors and their confidence intervals for interpretation on how they define the model.

```{r results='asis'}
confint(lm.fit2) %>%
  tidy() %>%
  add_column(coefficients = lm.fit2$coefficients, .after = 2) %>%
  knitr::kable()
```

In the above table, the X2.5 and X97.5 mark the lower and upper bounds for the 95% confidence interval of the regression coefficients. These are calculated using the standard errors of the estimated coefficients. As an example, for _age_, the estimated coefficient is ~34.49 and the 95% confidence interval lies between ~33.03 and ~35.95. This means that as per the model, keeping everything else fixed, an increase in 1 year of age will result in an increase of 34.49 in the value of $charges^{0.8}$ (since we transformed the outcome). However, this is an estimate and hence there is a scope for variation. This variation is accounted for by the confidence interval, denoting that about 95% of the times, the change in the value of $charges^{0.8}$ will be between 33.03 and 35.95, keeping everything else fixed.

This effect can be further viewed by plotting the effect using _effects_ function.

```{r}
#get effects for age
effs <- effect('age', mod = lm.fit2, 
       xlevels = list(age=min(insurance$age):max(insurance$age)))

#create a data frame for age and the fitted values, with upper and lower confidence bands
model.effs <- effs[c('x', 'lower', 'fit', 'upper')] %>%
  as.data.frame()

model.effs$fit <- model.effs$fit^(1/yTransformer)
model.effs$lower <- model.effs$lower^(1/yTransformer)
model.effs$upper <- model.effs$upper^(1/yTransformer)

ggplot(model.effs, aes(x = age, y = fit)) +
  theme_bw()+
  geom_smooth()+
  geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.5)
```

For an average value of other predictors, insurance charges increase with increase in age.

More interesting effects can be seen for the interaction between _bmi_ and _smoker_.

```{r}
#get effects for bmi*smoker
effs <- effect('bmi*smoker', mod = lm.fit2, 
       xlevels = list(bmi=min(insurance$bmi):max(insurance$bmi)))

#create a data frame for bmi*smoker and the fitted values, with upper and lower confidence bands
model.effs <- effs[c('x', 'lower', 'fit', 'upper')] %>%
  as.data.frame()

model.effs$fit <- model.effs$fit^(1/yTransformer)
model.effs$lower <- model.effs$lower^(1/yTransformer)
model.effs$upper <- model.effs$upper^(1/yTransformer)

ggplot(model.effs, aes(x = x.bmi, y = fit)) +
  facet_wrap(~x.smoker)+
  theme_bw()+
  geom_smooth(size=0.5)+
  geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.5)
```

Non-smokers, irrespective of their bmi, have mostly low insurance charges for an average value of other predictors. Smokers with low bmi have low insurance charges, though still higher than non-smokers with any value of bmi. Moreover, as their bmi increases, the insurance charges of smokers increases rapidly.

***

## Conclusion

The model we have built can be used for inference of how the different predictors influence the outcome. 

It is far from perfect. Their is still presence of non-linearity and non-constant variance of errors. Moreover, the outliers and leverage points should be analyzed further to find a better model. It may not (and most probably won't) give similar results when used to predict the outcome for new, unseen data. In order to use it for prediction, more concrete measures should be taken for ensuring the accuracy of the model, like cross-validation. 

It still helps by providing good estimations of the significant relations between the predictors and the outcome, which can themselves be used to summarize the data in a more useful and presentful way.

***

### Sources :

* [https://www.kaggle.com/mirichoi0218/insurance/home](https://www.kaggle.com/mirichoi0218/insurance/home){target="_blank"}
* An Introduction to Statistical Learning and Reasoning
* Wikipedia
* [https://www.statmethods.net/stats/rdiagnostics.html](https://www.statmethods.net/stats/rdiagnostics.html){target="_blank"}
* [https://www.statmethods.net/stats/regression.html](https://www.statmethods.net/stats/regression.html){target="_blank"}
* [https://datascienceplus.com/how-to-detect-heteroscedasticity-and-rectify-it/](https://datascienceplus.com/how-to-detect-heteroscedasticity-and-rectify-it/){target="_blank"}
* [http://www.statisticshowto.com/probability-and-statistics/f-statistic-value-test/](http://www.statisticshowto.com/probability-and-statistics/f-statistic-value-test/){target="_blank"}